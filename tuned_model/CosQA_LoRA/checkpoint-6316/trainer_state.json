{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 6316,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015832805573147563,
      "grad_norm": 13.265271293318941,
      "learning_rate": 5e-06,
      "loss": 0.872,
      "step": 10
    },
    {
      "epoch": 0.0031665611146295125,
      "grad_norm": 8.442714859812664,
      "learning_rate": 1e-05,
      "loss": 0.5775,
      "step": 20
    },
    {
      "epoch": 0.004749841671944269,
      "grad_norm": 6.439759385725908,
      "learning_rate": 1.5e-05,
      "loss": 0.3361,
      "step": 30
    },
    {
      "epoch": 0.006333122229259025,
      "grad_norm": 6.465284220738609,
      "learning_rate": 2e-05,
      "loss": 0.3356,
      "step": 40
    },
    {
      "epoch": 0.007916402786573781,
      "grad_norm": 3.7345283508736418,
      "learning_rate": 2.5e-05,
      "loss": 0.2311,
      "step": 50
    },
    {
      "epoch": 0.009499683343888538,
      "grad_norm": 8.157570684502602,
      "learning_rate": 3e-05,
      "loss": 0.3793,
      "step": 60
    },
    {
      "epoch": 0.011082963901203294,
      "grad_norm": 4.626903039315768,
      "learning_rate": 3.5e-05,
      "loss": 0.2886,
      "step": 70
    },
    {
      "epoch": 0.01266624445851805,
      "grad_norm": 4.548162848181571,
      "learning_rate": 4e-05,
      "loss": 0.2882,
      "step": 80
    },
    {
      "epoch": 0.014249525015832806,
      "grad_norm": 7.067100174520593,
      "learning_rate": 4.5e-05,
      "loss": 0.2588,
      "step": 90
    },
    {
      "epoch": 0.015832805573147563,
      "grad_norm": 5.795919416690127,
      "learning_rate": 5e-05,
      "loss": 0.3052,
      "step": 100
    },
    {
      "epoch": 0.01741608613046232,
      "grad_norm": 10.468458120348023,
      "learning_rate": 4.991956241956242e-05,
      "loss": 0.2234,
      "step": 110
    },
    {
      "epoch": 0.018999366687777075,
      "grad_norm": 4.454090535744682,
      "learning_rate": 4.983912483912484e-05,
      "loss": 0.275,
      "step": 120
    },
    {
      "epoch": 0.02058264724509183,
      "grad_norm": 8.167318149453621,
      "learning_rate": 4.9758687258687257e-05,
      "loss": 0.2602,
      "step": 130
    },
    {
      "epoch": 0.022165927802406588,
      "grad_norm": 6.966359639436982,
      "learning_rate": 4.967824967824968e-05,
      "loss": 0.2661,
      "step": 140
    },
    {
      "epoch": 0.023749208359721344,
      "grad_norm": 7.980939331221769,
      "learning_rate": 4.95978120978121e-05,
      "loss": 0.2387,
      "step": 150
    },
    {
      "epoch": 0.0253324889170361,
      "grad_norm": 7.0330667140860506,
      "learning_rate": 4.951737451737452e-05,
      "loss": 0.3035,
      "step": 160
    },
    {
      "epoch": 0.026915769474350856,
      "grad_norm": 4.627708466799365,
      "learning_rate": 4.943693693693694e-05,
      "loss": 0.1909,
      "step": 170
    },
    {
      "epoch": 0.028499050031665613,
      "grad_norm": 5.0372047962559385,
      "learning_rate": 4.935649935649936e-05,
      "loss": 0.2273,
      "step": 180
    },
    {
      "epoch": 0.03008233058898037,
      "grad_norm": 7.393683351166349,
      "learning_rate": 4.927606177606178e-05,
      "loss": 0.3203,
      "step": 190
    },
    {
      "epoch": 0.031665611146295125,
      "grad_norm": 6.022145411027743,
      "learning_rate": 4.9195624195624197e-05,
      "loss": 0.2134,
      "step": 200
    },
    {
      "epoch": 0.03324889170360988,
      "grad_norm": 5.802534194313022,
      "learning_rate": 4.911518661518662e-05,
      "loss": 0.1854,
      "step": 210
    },
    {
      "epoch": 0.03483217226092464,
      "grad_norm": 11.474861865573127,
      "learning_rate": 4.903474903474904e-05,
      "loss": 0.4044,
      "step": 220
    },
    {
      "epoch": 0.036415452818239394,
      "grad_norm": 7.434802936746337,
      "learning_rate": 4.895431145431146e-05,
      "loss": 0.2242,
      "step": 230
    },
    {
      "epoch": 0.03799873337555415,
      "grad_norm": 4.922787730040183,
      "learning_rate": 4.8873873873873876e-05,
      "loss": 0.2983,
      "step": 240
    },
    {
      "epoch": 0.039582013932868906,
      "grad_norm": 5.912829735495267,
      "learning_rate": 4.8793436293436294e-05,
      "loss": 0.22,
      "step": 250
    },
    {
      "epoch": 0.04116529449018366,
      "grad_norm": 4.188075011899602,
      "learning_rate": 4.871299871299871e-05,
      "loss": 0.2664,
      "step": 260
    },
    {
      "epoch": 0.04274857504749842,
      "grad_norm": 3.099921188583227,
      "learning_rate": 4.863256113256113e-05,
      "loss": 0.2013,
      "step": 270
    },
    {
      "epoch": 0.044331855604813175,
      "grad_norm": 6.699909141266074,
      "learning_rate": 4.8552123552123555e-05,
      "loss": 0.2674,
      "step": 280
    },
    {
      "epoch": 0.04591513616212793,
      "grad_norm": 6.743877884126879,
      "learning_rate": 4.847168597168598e-05,
      "loss": 0.2022,
      "step": 290
    },
    {
      "epoch": 0.04749841671944269,
      "grad_norm": 6.303561085191158,
      "learning_rate": 4.83912483912484e-05,
      "loss": 0.29,
      "step": 300
    },
    {
      "epoch": 0.049081697276757444,
      "grad_norm": 7.423357941898324,
      "learning_rate": 4.8310810810810816e-05,
      "loss": 0.259,
      "step": 310
    },
    {
      "epoch": 0.0506649778340722,
      "grad_norm": 7.613824878775974,
      "learning_rate": 4.8230373230373234e-05,
      "loss": 0.2067,
      "step": 320
    },
    {
      "epoch": 0.052248258391386956,
      "grad_norm": 4.689321545491079,
      "learning_rate": 4.814993564993565e-05,
      "loss": 0.3542,
      "step": 330
    },
    {
      "epoch": 0.05383153894870171,
      "grad_norm": 4.942223522767765,
      "learning_rate": 4.806949806949807e-05,
      "loss": 0.2355,
      "step": 340
    },
    {
      "epoch": 0.05541481950601647,
      "grad_norm": 3.05646901974359,
      "learning_rate": 4.798906048906049e-05,
      "loss": 0.2121,
      "step": 350
    },
    {
      "epoch": 0.056998100063331225,
      "grad_norm": 6.201691046180501,
      "learning_rate": 4.790862290862291e-05,
      "loss": 0.2399,
      "step": 360
    },
    {
      "epoch": 0.05858138062064598,
      "grad_norm": 9.69075203732033,
      "learning_rate": 4.782818532818533e-05,
      "loss": 0.1974,
      "step": 370
    },
    {
      "epoch": 0.06016466117796074,
      "grad_norm": 5.977894433895012,
      "learning_rate": 4.774774774774775e-05,
      "loss": 0.1978,
      "step": 380
    },
    {
      "epoch": 0.061747941735275494,
      "grad_norm": 4.472403999778133,
      "learning_rate": 4.766731016731017e-05,
      "loss": 0.1747,
      "step": 390
    },
    {
      "epoch": 0.06333122229259025,
      "grad_norm": 10.544717661305144,
      "learning_rate": 4.758687258687259e-05,
      "loss": 0.2045,
      "step": 400
    },
    {
      "epoch": 0.064914502849905,
      "grad_norm": 8.201670723025021,
      "learning_rate": 4.750643500643501e-05,
      "loss": 0.1666,
      "step": 410
    },
    {
      "epoch": 0.06649778340721976,
      "grad_norm": 13.048719227814205,
      "learning_rate": 4.742599742599743e-05,
      "loss": 0.2813,
      "step": 420
    },
    {
      "epoch": 0.06808106396453452,
      "grad_norm": 14.128568755472898,
      "learning_rate": 4.734555984555985e-05,
      "loss": 0.2607,
      "step": 430
    },
    {
      "epoch": 0.06966434452184928,
      "grad_norm": 5.08586793257193,
      "learning_rate": 4.726512226512227e-05,
      "loss": 0.2842,
      "step": 440
    },
    {
      "epoch": 0.07124762507916403,
      "grad_norm": 6.064240510564427,
      "learning_rate": 4.718468468468469e-05,
      "loss": 0.2364,
      "step": 450
    },
    {
      "epoch": 0.07283090563647879,
      "grad_norm": 4.646429163383093,
      "learning_rate": 4.710424710424711e-05,
      "loss": 0.1967,
      "step": 460
    },
    {
      "epoch": 0.07441418619379354,
      "grad_norm": 4.842139191585016,
      "learning_rate": 4.7023809523809525e-05,
      "loss": 0.2093,
      "step": 470
    },
    {
      "epoch": 0.0759974667511083,
      "grad_norm": 16.56103324873255,
      "learning_rate": 4.694337194337194e-05,
      "loss": 0.2003,
      "step": 480
    },
    {
      "epoch": 0.07758074730842306,
      "grad_norm": 10.684492078319852,
      "learning_rate": 4.686293436293436e-05,
      "loss": 0.211,
      "step": 490
    },
    {
      "epoch": 0.07916402786573781,
      "grad_norm": 5.11970954071127,
      "learning_rate": 4.6782496782496786e-05,
      "loss": 0.295,
      "step": 500
    },
    {
      "epoch": 0.07916402786573781,
      "eval_loss": 0.26940351724624634,
      "eval_runtime": 306.3845,
      "eval_samples_per_second": 9.743,
      "eval_steps_per_second": 1.625,
      "step": 500
    },
    {
      "epoch": 0.08074730842305257,
      "grad_norm": 4.987977068654863,
      "learning_rate": 4.6702059202059204e-05,
      "loss": 0.1914,
      "step": 510
    },
    {
      "epoch": 0.08233058898036733,
      "grad_norm": 7.443851868054933,
      "learning_rate": 4.662162162162162e-05,
      "loss": 0.2066,
      "step": 520
    },
    {
      "epoch": 0.08391386953768208,
      "grad_norm": 6.979809388889112,
      "learning_rate": 4.654118404118405e-05,
      "loss": 0.1978,
      "step": 530
    },
    {
      "epoch": 0.08549715009499684,
      "grad_norm": 4.824095788539232,
      "learning_rate": 4.6460746460746465e-05,
      "loss": 0.2198,
      "step": 540
    },
    {
      "epoch": 0.0870804306523116,
      "grad_norm": 7.807995771875956,
      "learning_rate": 4.638030888030888e-05,
      "loss": 0.216,
      "step": 550
    },
    {
      "epoch": 0.08866371120962635,
      "grad_norm": 3.3799097588585623,
      "learning_rate": 4.62998712998713e-05,
      "loss": 0.2357,
      "step": 560
    },
    {
      "epoch": 0.0902469917669411,
      "grad_norm": 7.236565079200754,
      "learning_rate": 4.621943371943372e-05,
      "loss": 0.2209,
      "step": 570
    },
    {
      "epoch": 0.09183027232425586,
      "grad_norm": 4.354680640930753,
      "learning_rate": 4.6138996138996144e-05,
      "loss": 0.166,
      "step": 580
    },
    {
      "epoch": 0.09341355288157062,
      "grad_norm": 8.323938287157013,
      "learning_rate": 4.605855855855856e-05,
      "loss": 0.1444,
      "step": 590
    },
    {
      "epoch": 0.09499683343888538,
      "grad_norm": 8.555397819397093,
      "learning_rate": 4.597812097812098e-05,
      "loss": 0.1746,
      "step": 600
    },
    {
      "epoch": 0.09658011399620013,
      "grad_norm": 22.35795412807193,
      "learning_rate": 4.58976833976834e-05,
      "loss": 0.1361,
      "step": 610
    },
    {
      "epoch": 0.09816339455351489,
      "grad_norm": 8.370728271594713,
      "learning_rate": 4.5817245817245816e-05,
      "loss": 0.2456,
      "step": 620
    },
    {
      "epoch": 0.09974667511082964,
      "grad_norm": 10.5465404316147,
      "learning_rate": 4.5736808236808234e-05,
      "loss": 0.2315,
      "step": 630
    },
    {
      "epoch": 0.1013299556681444,
      "grad_norm": 10.137778424862203,
      "learning_rate": 4.565637065637066e-05,
      "loss": 0.1806,
      "step": 640
    },
    {
      "epoch": 0.10291323622545916,
      "grad_norm": 6.291588317517994,
      "learning_rate": 4.5575933075933084e-05,
      "loss": 0.1855,
      "step": 650
    },
    {
      "epoch": 0.10449651678277391,
      "grad_norm": 10.60488135746728,
      "learning_rate": 4.54954954954955e-05,
      "loss": 0.2315,
      "step": 660
    },
    {
      "epoch": 0.10607979734008867,
      "grad_norm": 4.303790599621527,
      "learning_rate": 4.541505791505792e-05,
      "loss": 0.2186,
      "step": 670
    },
    {
      "epoch": 0.10766307789740343,
      "grad_norm": 7.704711007968346,
      "learning_rate": 4.533462033462034e-05,
      "loss": 0.2664,
      "step": 680
    },
    {
      "epoch": 0.10924635845471818,
      "grad_norm": 8.475267218143939,
      "learning_rate": 4.5254182754182756e-05,
      "loss": 0.2127,
      "step": 690
    },
    {
      "epoch": 0.11082963901203294,
      "grad_norm": 4.47055636090469,
      "learning_rate": 4.5173745173745174e-05,
      "loss": 0.2614,
      "step": 700
    },
    {
      "epoch": 0.1124129195693477,
      "grad_norm": 11.92108271654742,
      "learning_rate": 4.509330759330759e-05,
      "loss": 0.1897,
      "step": 710
    },
    {
      "epoch": 0.11399620012666245,
      "grad_norm": 5.341409628045316,
      "learning_rate": 4.501287001287002e-05,
      "loss": 0.1765,
      "step": 720
    },
    {
      "epoch": 0.1155794806839772,
      "grad_norm": 7.771820605631801,
      "learning_rate": 4.4932432432432435e-05,
      "loss": 0.2742,
      "step": 730
    },
    {
      "epoch": 0.11716276124129196,
      "grad_norm": 7.033684203208087,
      "learning_rate": 4.485199485199485e-05,
      "loss": 0.1804,
      "step": 740
    },
    {
      "epoch": 0.11874604179860672,
      "grad_norm": 13.190945866633117,
      "learning_rate": 4.477155727155727e-05,
      "loss": 0.1466,
      "step": 750
    },
    {
      "epoch": 0.12032932235592148,
      "grad_norm": 7.713738795476159,
      "learning_rate": 4.4691119691119696e-05,
      "loss": 0.1699,
      "step": 760
    },
    {
      "epoch": 0.12191260291323623,
      "grad_norm": 18.1604062180312,
      "learning_rate": 4.4610682110682114e-05,
      "loss": 0.2253,
      "step": 770
    },
    {
      "epoch": 0.12349588347055099,
      "grad_norm": 5.618915573243496,
      "learning_rate": 4.453024453024453e-05,
      "loss": 0.0932,
      "step": 780
    },
    {
      "epoch": 0.12507916402786573,
      "grad_norm": 1.60943588586095,
      "learning_rate": 4.444980694980696e-05,
      "loss": 0.1301,
      "step": 790
    },
    {
      "epoch": 0.1266624445851805,
      "grad_norm": 10.963373067228465,
      "learning_rate": 4.4369369369369375e-05,
      "loss": 0.299,
      "step": 800
    },
    {
      "epoch": 0.12824572514249524,
      "grad_norm": 6.148384452697622,
      "learning_rate": 4.428893178893179e-05,
      "loss": 0.2578,
      "step": 810
    },
    {
      "epoch": 0.12982900569981,
      "grad_norm": 7.566144081028514,
      "learning_rate": 4.420849420849421e-05,
      "loss": 0.1676,
      "step": 820
    },
    {
      "epoch": 0.13141228625712476,
      "grad_norm": 13.634112469359655,
      "learning_rate": 4.412805662805663e-05,
      "loss": 0.2547,
      "step": 830
    },
    {
      "epoch": 0.13299556681443953,
      "grad_norm": 1.3856314251259936,
      "learning_rate": 4.404761904761905e-05,
      "loss": 0.1799,
      "step": 840
    },
    {
      "epoch": 0.13457884737175427,
      "grad_norm": 4.834546013774381,
      "learning_rate": 4.3967181467181465e-05,
      "loss": 0.2079,
      "step": 850
    },
    {
      "epoch": 0.13616212792906904,
      "grad_norm": 23.772423368116442,
      "learning_rate": 4.3886743886743883e-05,
      "loss": 0.2946,
      "step": 860
    },
    {
      "epoch": 0.13774540848638378,
      "grad_norm": 5.256147872168511,
      "learning_rate": 4.380630630630631e-05,
      "loss": 0.239,
      "step": 870
    },
    {
      "epoch": 0.13932868904369855,
      "grad_norm": 6.44611086438008,
      "learning_rate": 4.3725868725868726e-05,
      "loss": 0.1832,
      "step": 880
    },
    {
      "epoch": 0.1409119696010133,
      "grad_norm": 8.065065434114157,
      "learning_rate": 4.364543114543115e-05,
      "loss": 0.2233,
      "step": 890
    },
    {
      "epoch": 0.14249525015832806,
      "grad_norm": 10.142199565978418,
      "learning_rate": 4.356499356499357e-05,
      "loss": 0.2165,
      "step": 900
    },
    {
      "epoch": 0.1440785307156428,
      "grad_norm": 10.410202054894539,
      "learning_rate": 4.348455598455599e-05,
      "loss": 0.2507,
      "step": 910
    },
    {
      "epoch": 0.14566181127295758,
      "grad_norm": 8.246340778216041,
      "learning_rate": 4.3404118404118405e-05,
      "loss": 0.1951,
      "step": 920
    },
    {
      "epoch": 0.14724509183027232,
      "grad_norm": 4.555301160351423,
      "learning_rate": 4.3323680823680823e-05,
      "loss": 0.1237,
      "step": 930
    },
    {
      "epoch": 0.1488283723875871,
      "grad_norm": 8.420354759009749,
      "learning_rate": 4.324324324324325e-05,
      "loss": 0.205,
      "step": 940
    },
    {
      "epoch": 0.15041165294490183,
      "grad_norm": 6.414474343983234,
      "learning_rate": 4.3162805662805666e-05,
      "loss": 0.1606,
      "step": 950
    },
    {
      "epoch": 0.1519949335022166,
      "grad_norm": 5.972766423628226,
      "learning_rate": 4.3082368082368084e-05,
      "loss": 0.1555,
      "step": 960
    },
    {
      "epoch": 0.15357821405953134,
      "grad_norm": 20.45593811226301,
      "learning_rate": 4.30019305019305e-05,
      "loss": 0.1364,
      "step": 970
    },
    {
      "epoch": 0.1551614946168461,
      "grad_norm": 14.627212748271734,
      "learning_rate": 4.292149292149292e-05,
      "loss": 0.2182,
      "step": 980
    },
    {
      "epoch": 0.15674477517416086,
      "grad_norm": 10.671858914587988,
      "learning_rate": 4.284105534105534e-05,
      "loss": 0.2132,
      "step": 990
    },
    {
      "epoch": 0.15832805573147563,
      "grad_norm": 11.683683210749109,
      "learning_rate": 4.276061776061776e-05,
      "loss": 0.2354,
      "step": 1000
    },
    {
      "epoch": 0.15832805573147563,
      "eval_loss": 0.25301244854927063,
      "eval_runtime": 305.2112,
      "eval_samples_per_second": 9.78,
      "eval_steps_per_second": 1.632,
      "step": 1000
    },
    {
      "epoch": 0.15991133628879037,
      "grad_norm": 6.08312688329141,
      "learning_rate": 4.268018018018018e-05,
      "loss": 0.1655,
      "step": 1010
    },
    {
      "epoch": 0.16149461684610514,
      "grad_norm": 14.263418204517215,
      "learning_rate": 4.2599742599742606e-05,
      "loss": 0.2522,
      "step": 1020
    },
    {
      "epoch": 0.16307789740341988,
      "grad_norm": 3.738498725265402,
      "learning_rate": 4.2519305019305024e-05,
      "loss": 0.1839,
      "step": 1030
    },
    {
      "epoch": 0.16466117796073465,
      "grad_norm": 11.480653494600642,
      "learning_rate": 4.243886743886744e-05,
      "loss": 0.242,
      "step": 1040
    },
    {
      "epoch": 0.1662444585180494,
      "grad_norm": 8.554755279411856,
      "learning_rate": 4.235842985842986e-05,
      "loss": 0.1831,
      "step": 1050
    },
    {
      "epoch": 0.16782773907536416,
      "grad_norm": 19.183265641283878,
      "learning_rate": 4.227799227799228e-05,
      "loss": 0.1447,
      "step": 1060
    },
    {
      "epoch": 0.1694110196326789,
      "grad_norm": 2.0364234186351866,
      "learning_rate": 4.2197554697554697e-05,
      "loss": 0.142,
      "step": 1070
    },
    {
      "epoch": 0.17099430018999368,
      "grad_norm": 7.898474688494302,
      "learning_rate": 4.2117117117117115e-05,
      "loss": 0.126,
      "step": 1080
    },
    {
      "epoch": 0.17257758074730842,
      "grad_norm": 3.5393791141314326,
      "learning_rate": 4.203667953667954e-05,
      "loss": 0.1739,
      "step": 1090
    },
    {
      "epoch": 0.1741608613046232,
      "grad_norm": 23.31868974672261,
      "learning_rate": 4.195624195624196e-05,
      "loss": 0.1596,
      "step": 1100
    },
    {
      "epoch": 0.17574414186193793,
      "grad_norm": 6.640902452843559,
      "learning_rate": 4.1875804375804376e-05,
      "loss": 0.2701,
      "step": 1110
    },
    {
      "epoch": 0.1773274224192527,
      "grad_norm": 25.775439686174607,
      "learning_rate": 4.17953667953668e-05,
      "loss": 0.3224,
      "step": 1120
    },
    {
      "epoch": 0.17891070297656744,
      "grad_norm": 9.40328131685561,
      "learning_rate": 4.171492921492922e-05,
      "loss": 0.3298,
      "step": 1130
    },
    {
      "epoch": 0.1804939835338822,
      "grad_norm": 5.515506732310292,
      "learning_rate": 4.1634491634491637e-05,
      "loss": 0.1779,
      "step": 1140
    },
    {
      "epoch": 0.18207726409119696,
      "grad_norm": 2.6615250212693424,
      "learning_rate": 4.1554054054054055e-05,
      "loss": 0.266,
      "step": 1150
    },
    {
      "epoch": 0.18366054464851173,
      "grad_norm": 6.267903738815134,
      "learning_rate": 4.147361647361648e-05,
      "loss": 0.219,
      "step": 1160
    },
    {
      "epoch": 0.18524382520582647,
      "grad_norm": 5.398451279265157,
      "learning_rate": 4.13931788931789e-05,
      "loss": 0.2585,
      "step": 1170
    },
    {
      "epoch": 0.18682710576314124,
      "grad_norm": 3.854745907257894,
      "learning_rate": 4.1312741312741316e-05,
      "loss": 0.2188,
      "step": 1180
    },
    {
      "epoch": 0.18841038632045598,
      "grad_norm": 2.9188660821151533,
      "learning_rate": 4.1232303732303734e-05,
      "loss": 0.1654,
      "step": 1190
    },
    {
      "epoch": 0.18999366687777075,
      "grad_norm": 4.338187628776079,
      "learning_rate": 4.115186615186615e-05,
      "loss": 0.156,
      "step": 1200
    },
    {
      "epoch": 0.1915769474350855,
      "grad_norm": 11.206379054741754,
      "learning_rate": 4.107142857142857e-05,
      "loss": 0.1969,
      "step": 1210
    },
    {
      "epoch": 0.19316022799240026,
      "grad_norm": 1.4108724703752138,
      "learning_rate": 4.099099099099099e-05,
      "loss": 0.1792,
      "step": 1220
    },
    {
      "epoch": 0.194743508549715,
      "grad_norm": 2.640416153995062,
      "learning_rate": 4.091055341055341e-05,
      "loss": 0.143,
      "step": 1230
    },
    {
      "epoch": 0.19632678910702978,
      "grad_norm": 10.189061331898799,
      "learning_rate": 4.083011583011583e-05,
      "loss": 0.2119,
      "step": 1240
    },
    {
      "epoch": 0.19791006966434452,
      "grad_norm": 10.199032861074109,
      "learning_rate": 4.0749678249678256e-05,
      "loss": 0.2558,
      "step": 1250
    },
    {
      "epoch": 0.1994933502216593,
      "grad_norm": 4.3117575628320814,
      "learning_rate": 4.0669240669240674e-05,
      "loss": 0.2298,
      "step": 1260
    },
    {
      "epoch": 0.20107663077897403,
      "grad_norm": 13.84792731575317,
      "learning_rate": 4.058880308880309e-05,
      "loss": 0.2022,
      "step": 1270
    },
    {
      "epoch": 0.2026599113362888,
      "grad_norm": 4.818474763958334,
      "learning_rate": 4.050836550836551e-05,
      "loss": 0.2051,
      "step": 1280
    },
    {
      "epoch": 0.20424319189360354,
      "grad_norm": 6.47133536895744,
      "learning_rate": 4.042792792792793e-05,
      "loss": 0.1969,
      "step": 1290
    },
    {
      "epoch": 0.2058264724509183,
      "grad_norm": 4.904893432455005,
      "learning_rate": 4.034749034749035e-05,
      "loss": 0.199,
      "step": 1300
    },
    {
      "epoch": 0.20740975300823306,
      "grad_norm": 3.6891107435679,
      "learning_rate": 4.026705276705277e-05,
      "loss": 0.1613,
      "step": 1310
    },
    {
      "epoch": 0.20899303356554783,
      "grad_norm": 1.2792534973711012,
      "learning_rate": 4.018661518661519e-05,
      "loss": 0.2113,
      "step": 1320
    },
    {
      "epoch": 0.21057631412286257,
      "grad_norm": 1.6405277041694795,
      "learning_rate": 4.010617760617761e-05,
      "loss": 0.218,
      "step": 1330
    },
    {
      "epoch": 0.21215959468017734,
      "grad_norm": 4.4078811980921335,
      "learning_rate": 4.0025740025740025e-05,
      "loss": 0.2071,
      "step": 1340
    },
    {
      "epoch": 0.21374287523749208,
      "grad_norm": 7.399556285855191,
      "learning_rate": 3.994530244530244e-05,
      "loss": 0.195,
      "step": 1350
    },
    {
      "epoch": 0.21532615579480685,
      "grad_norm": 3.635686447203996,
      "learning_rate": 3.986486486486487e-05,
      "loss": 0.1806,
      "step": 1360
    },
    {
      "epoch": 0.2169094363521216,
      "grad_norm": 10.422742816075747,
      "learning_rate": 3.9784427284427286e-05,
      "loss": 0.1693,
      "step": 1370
    },
    {
      "epoch": 0.21849271690943636,
      "grad_norm": 4.792194879138739,
      "learning_rate": 3.970398970398971e-05,
      "loss": 0.1601,
      "step": 1380
    },
    {
      "epoch": 0.2200759974667511,
      "grad_norm": 0.5435335528712534,
      "learning_rate": 3.962355212355213e-05,
      "loss": 0.1623,
      "step": 1390
    },
    {
      "epoch": 0.22165927802406588,
      "grad_norm": 8.300939336729929,
      "learning_rate": 3.954311454311455e-05,
      "loss": 0.2194,
      "step": 1400
    },
    {
      "epoch": 0.22324255858138062,
      "grad_norm": 6.127811331650853,
      "learning_rate": 3.9462676962676965e-05,
      "loss": 0.1475,
      "step": 1410
    },
    {
      "epoch": 0.2248258391386954,
      "grad_norm": 9.718092917648745,
      "learning_rate": 3.938223938223938e-05,
      "loss": 0.1415,
      "step": 1420
    },
    {
      "epoch": 0.22640911969601013,
      "grad_norm": 11.102466522283144,
      "learning_rate": 3.93018018018018e-05,
      "loss": 0.1303,
      "step": 1430
    },
    {
      "epoch": 0.2279924002533249,
      "grad_norm": 4.156680665613344,
      "learning_rate": 3.922136422136422e-05,
      "loss": 0.2288,
      "step": 1440
    },
    {
      "epoch": 0.22957568081063964,
      "grad_norm": 11.505560152757054,
      "learning_rate": 3.9140926640926644e-05,
      "loss": 0.226,
      "step": 1450
    },
    {
      "epoch": 0.2311589613679544,
      "grad_norm": 1.543084248915891,
      "learning_rate": 3.906048906048906e-05,
      "loss": 0.2421,
      "step": 1460
    },
    {
      "epoch": 0.23274224192526916,
      "grad_norm": 0.698012478263282,
      "learning_rate": 3.898005148005148e-05,
      "loss": 0.1581,
      "step": 1470
    },
    {
      "epoch": 0.23432552248258393,
      "grad_norm": 3.1095932160295896,
      "learning_rate": 3.8899613899613905e-05,
      "loss": 0.144,
      "step": 1480
    },
    {
      "epoch": 0.23590880303989867,
      "grad_norm": 4.140253935687706,
      "learning_rate": 3.881917631917632e-05,
      "loss": 0.1395,
      "step": 1490
    },
    {
      "epoch": 0.23749208359721344,
      "grad_norm": 6.5703220798272906,
      "learning_rate": 3.873873873873874e-05,
      "loss": 0.1527,
      "step": 1500
    },
    {
      "epoch": 0.23749208359721344,
      "eval_loss": 0.25240692496299744,
      "eval_runtime": 304.1829,
      "eval_samples_per_second": 9.813,
      "eval_steps_per_second": 1.637,
      "step": 1500
    },
    {
      "epoch": 0.23907536415452818,
      "grad_norm": 6.488070471346786,
      "learning_rate": 3.865830115830116e-05,
      "loss": 0.2491,
      "step": 1510
    },
    {
      "epoch": 0.24065864471184295,
      "grad_norm": 10.179729470366281,
      "learning_rate": 3.8577863577863584e-05,
      "loss": 0.1679,
      "step": 1520
    },
    {
      "epoch": 0.2422419252691577,
      "grad_norm": 2.036088667649451,
      "learning_rate": 3.8497425997426e-05,
      "loss": 0.1255,
      "step": 1530
    },
    {
      "epoch": 0.24382520582647246,
      "grad_norm": 6.086564586294535,
      "learning_rate": 3.841698841698842e-05,
      "loss": 0.2206,
      "step": 1540
    },
    {
      "epoch": 0.2454084863837872,
      "grad_norm": 0.5346696966831382,
      "learning_rate": 3.833655083655084e-05,
      "loss": 0.2155,
      "step": 1550
    },
    {
      "epoch": 0.24699176694110198,
      "grad_norm": 4.656772660997199,
      "learning_rate": 3.8256113256113256e-05,
      "loss": 0.1521,
      "step": 1560
    },
    {
      "epoch": 0.24857504749841672,
      "grad_norm": 5.508892499942954,
      "learning_rate": 3.8175675675675674e-05,
      "loss": 0.1409,
      "step": 1570
    },
    {
      "epoch": 0.25015832805573146,
      "grad_norm": 12.99633033016556,
      "learning_rate": 3.809523809523809e-05,
      "loss": 0.1966,
      "step": 1580
    },
    {
      "epoch": 0.25174160861304623,
      "grad_norm": 7.5071788762607605,
      "learning_rate": 3.801480051480052e-05,
      "loss": 0.1798,
      "step": 1590
    },
    {
      "epoch": 0.253324889170361,
      "grad_norm": 10.446026819722386,
      "learning_rate": 3.7934362934362935e-05,
      "loss": 0.2438,
      "step": 1600
    },
    {
      "epoch": 0.25490816972767577,
      "grad_norm": 7.455792411583033,
      "learning_rate": 3.785392535392536e-05,
      "loss": 0.2118,
      "step": 1610
    },
    {
      "epoch": 0.2564914502849905,
      "grad_norm": 6.599741450938686,
      "learning_rate": 3.777348777348778e-05,
      "loss": 0.144,
      "step": 1620
    },
    {
      "epoch": 0.25807473084230526,
      "grad_norm": 1.3178226989250117,
      "learning_rate": 3.7693050193050196e-05,
      "loss": 0.1451,
      "step": 1630
    },
    {
      "epoch": 0.25965801139962,
      "grad_norm": 10.240150371281901,
      "learning_rate": 3.7612612612612614e-05,
      "loss": 0.2444,
      "step": 1640
    },
    {
      "epoch": 0.2612412919569348,
      "grad_norm": 8.863372923308413,
      "learning_rate": 3.753217503217503e-05,
      "loss": 0.1213,
      "step": 1650
    },
    {
      "epoch": 0.2628245725142495,
      "grad_norm": 3.4020059726669385,
      "learning_rate": 3.745173745173745e-05,
      "loss": 0.1513,
      "step": 1660
    },
    {
      "epoch": 0.2644078530715643,
      "grad_norm": 7.62494621883249,
      "learning_rate": 3.7371299871299875e-05,
      "loss": 0.3239,
      "step": 1670
    },
    {
      "epoch": 0.26599113362887905,
      "grad_norm": 2.7899234415303997,
      "learning_rate": 3.729086229086229e-05,
      "loss": 0.199,
      "step": 1680
    },
    {
      "epoch": 0.2675744141861938,
      "grad_norm": 2.463386987799553,
      "learning_rate": 3.721042471042471e-05,
      "loss": 0.1789,
      "step": 1690
    },
    {
      "epoch": 0.26915769474350854,
      "grad_norm": 5.347946571511189,
      "learning_rate": 3.712998712998713e-05,
      "loss": 0.1933,
      "step": 1700
    },
    {
      "epoch": 0.2707409753008233,
      "grad_norm": 7.418824157087112,
      "learning_rate": 3.704954954954955e-05,
      "loss": 0.1388,
      "step": 1710
    },
    {
      "epoch": 0.2723242558581381,
      "grad_norm": 16.670946004166293,
      "learning_rate": 3.696911196911197e-05,
      "loss": 0.182,
      "step": 1720
    },
    {
      "epoch": 0.27390753641545285,
      "grad_norm": 5.324930167299952,
      "learning_rate": 3.688867438867439e-05,
      "loss": 0.2029,
      "step": 1730
    },
    {
      "epoch": 0.27549081697276756,
      "grad_norm": 5.99758067628155,
      "learning_rate": 3.6808236808236815e-05,
      "loss": 0.1879,
      "step": 1740
    },
    {
      "epoch": 0.27707409753008233,
      "grad_norm": 5.2202224938341315,
      "learning_rate": 3.672779922779923e-05,
      "loss": 0.2305,
      "step": 1750
    },
    {
      "epoch": 0.2786573780873971,
      "grad_norm": 4.63194948749309,
      "learning_rate": 3.664736164736165e-05,
      "loss": 0.1541,
      "step": 1760
    },
    {
      "epoch": 0.2802406586447118,
      "grad_norm": 5.254664937089159,
      "learning_rate": 3.656692406692407e-05,
      "loss": 0.2076,
      "step": 1770
    },
    {
      "epoch": 0.2818239392020266,
      "grad_norm": 7.533097796288541,
      "learning_rate": 3.648648648648649e-05,
      "loss": 0.2344,
      "step": 1780
    },
    {
      "epoch": 0.28340721975934136,
      "grad_norm": 5.3381762967259485,
      "learning_rate": 3.6406048906048905e-05,
      "loss": 0.1857,
      "step": 1790
    },
    {
      "epoch": 0.2849905003166561,
      "grad_norm": 11.99000196391671,
      "learning_rate": 3.6325611325611323e-05,
      "loss": 0.2053,
      "step": 1800
    },
    {
      "epoch": 0.28657378087397084,
      "grad_norm": 4.084899887296309,
      "learning_rate": 3.624517374517375e-05,
      "loss": 0.129,
      "step": 1810
    },
    {
      "epoch": 0.2881570614312856,
      "grad_norm": 15.552255468146232,
      "learning_rate": 3.6164736164736166e-05,
      "loss": 0.1732,
      "step": 1820
    },
    {
      "epoch": 0.2897403419886004,
      "grad_norm": 6.072558995003897,
      "learning_rate": 3.6084298584298584e-05,
      "loss": 0.2562,
      "step": 1830
    },
    {
      "epoch": 0.29132362254591515,
      "grad_norm": 6.536356558646994,
      "learning_rate": 3.6003861003861e-05,
      "loss": 0.2288,
      "step": 1840
    },
    {
      "epoch": 0.29290690310322987,
      "grad_norm": 19.94595358595635,
      "learning_rate": 3.592342342342343e-05,
      "loss": 0.2034,
      "step": 1850
    },
    {
      "epoch": 0.29449018366054464,
      "grad_norm": 13.068418777187143,
      "learning_rate": 3.5842985842985845e-05,
      "loss": 0.1952,
      "step": 1860
    },
    {
      "epoch": 0.2960734642178594,
      "grad_norm": 4.984529403160511,
      "learning_rate": 3.5762548262548263e-05,
      "loss": 0.1578,
      "step": 1870
    },
    {
      "epoch": 0.2976567447751742,
      "grad_norm": 3.2403144297675444,
      "learning_rate": 3.568211068211069e-05,
      "loss": 0.2243,
      "step": 1880
    },
    {
      "epoch": 0.2992400253324889,
      "grad_norm": 6.580687107746318,
      "learning_rate": 3.5601673101673106e-05,
      "loss": 0.2134,
      "step": 1890
    },
    {
      "epoch": 0.30082330588980366,
      "grad_norm": 2.4279065243734097,
      "learning_rate": 3.5521235521235524e-05,
      "loss": 0.1943,
      "step": 1900
    },
    {
      "epoch": 0.30240658644711843,
      "grad_norm": 9.331352250700869,
      "learning_rate": 3.544079794079794e-05,
      "loss": 0.1856,
      "step": 1910
    },
    {
      "epoch": 0.3039898670044332,
      "grad_norm": 8.78169048467063,
      "learning_rate": 3.536036036036036e-05,
      "loss": 0.2005,
      "step": 1920
    },
    {
      "epoch": 0.3055731475617479,
      "grad_norm": 2.5149336155042077,
      "learning_rate": 3.527992277992278e-05,
      "loss": 0.2625,
      "step": 1930
    },
    {
      "epoch": 0.3071564281190627,
      "grad_norm": 3.0768842181172853,
      "learning_rate": 3.51994851994852e-05,
      "loss": 0.1049,
      "step": 1940
    },
    {
      "epoch": 0.30873970867637746,
      "grad_norm": 2.509961594823517,
      "learning_rate": 3.511904761904762e-05,
      "loss": 0.1925,
      "step": 1950
    },
    {
      "epoch": 0.3103229892336922,
      "grad_norm": 8.929812239395595,
      "learning_rate": 3.503861003861004e-05,
      "loss": 0.1786,
      "step": 1960
    },
    {
      "epoch": 0.31190626979100694,
      "grad_norm": 8.607943711844836,
      "learning_rate": 3.4958172458172464e-05,
      "loss": 0.2056,
      "step": 1970
    },
    {
      "epoch": 0.3134895503483217,
      "grad_norm": 4.429778567392272,
      "learning_rate": 3.487773487773488e-05,
      "loss": 0.0957,
      "step": 1980
    },
    {
      "epoch": 0.3150728309056365,
      "grad_norm": 4.41011992068739,
      "learning_rate": 3.47972972972973e-05,
      "loss": 0.226,
      "step": 1990
    },
    {
      "epoch": 0.31665611146295125,
      "grad_norm": 3.3284136866340037,
      "learning_rate": 3.471685971685972e-05,
      "loss": 0.1979,
      "step": 2000
    },
    {
      "epoch": 0.31665611146295125,
      "eval_loss": 0.24836134910583496,
      "eval_runtime": 303.3042,
      "eval_samples_per_second": 9.842,
      "eval_steps_per_second": 1.642,
      "step": 2000
    },
    {
      "epoch": 0.31823939202026597,
      "grad_norm": 13.009103961790446,
      "learning_rate": 3.4636422136422137e-05,
      "loss": 0.1664,
      "step": 2010
    },
    {
      "epoch": 0.31982267257758074,
      "grad_norm": 9.440314473232108,
      "learning_rate": 3.4555984555984555e-05,
      "loss": 0.1183,
      "step": 2020
    },
    {
      "epoch": 0.3214059531348955,
      "grad_norm": 13.222764724444271,
      "learning_rate": 3.447554697554698e-05,
      "loss": 0.323,
      "step": 2030
    },
    {
      "epoch": 0.3229892336922103,
      "grad_norm": 4.618324204409133,
      "learning_rate": 3.43951093951094e-05,
      "loss": 0.1503,
      "step": 2040
    },
    {
      "epoch": 0.324572514249525,
      "grad_norm": 5.015927314560207,
      "learning_rate": 3.4314671814671816e-05,
      "loss": 0.2288,
      "step": 2050
    },
    {
      "epoch": 0.32615579480683976,
      "grad_norm": 11.835047924491054,
      "learning_rate": 3.4234234234234234e-05,
      "loss": 0.1265,
      "step": 2060
    },
    {
      "epoch": 0.32773907536415453,
      "grad_norm": 2.4548726265178726,
      "learning_rate": 3.415379665379665e-05,
      "loss": 0.2,
      "step": 2070
    },
    {
      "epoch": 0.3293223559214693,
      "grad_norm": 3.3639750368732013,
      "learning_rate": 3.4073359073359077e-05,
      "loss": 0.2709,
      "step": 2080
    },
    {
      "epoch": 0.330905636478784,
      "grad_norm": 5.614464791013153,
      "learning_rate": 3.3992921492921495e-05,
      "loss": 0.1756,
      "step": 2090
    },
    {
      "epoch": 0.3324889170360988,
      "grad_norm": 4.103098438232166,
      "learning_rate": 3.391248391248392e-05,
      "loss": 0.2086,
      "step": 2100
    },
    {
      "epoch": 0.33407219759341356,
      "grad_norm": 9.844507194340103,
      "learning_rate": 3.383204633204634e-05,
      "loss": 0.1739,
      "step": 2110
    },
    {
      "epoch": 0.3356554781507283,
      "grad_norm": 2.591134096563138,
      "learning_rate": 3.3751608751608756e-05,
      "loss": 0.1247,
      "step": 2120
    },
    {
      "epoch": 0.33723875870804304,
      "grad_norm": 2.4977678824670315,
      "learning_rate": 3.3671171171171174e-05,
      "loss": 0.1212,
      "step": 2130
    },
    {
      "epoch": 0.3388220392653578,
      "grad_norm": 5.422065665550628,
      "learning_rate": 3.359073359073359e-05,
      "loss": 0.1492,
      "step": 2140
    },
    {
      "epoch": 0.3404053198226726,
      "grad_norm": 5.988420438797417,
      "learning_rate": 3.351029601029601e-05,
      "loss": 0.2147,
      "step": 2150
    },
    {
      "epoch": 0.34198860037998735,
      "grad_norm": 3.404119962996772,
      "learning_rate": 3.342985842985843e-05,
      "loss": 0.2143,
      "step": 2160
    },
    {
      "epoch": 0.34357188093730207,
      "grad_norm": 24.86404070177515,
      "learning_rate": 3.334942084942085e-05,
      "loss": 0.1481,
      "step": 2170
    },
    {
      "epoch": 0.34515516149461684,
      "grad_norm": 6.768546444974962,
      "learning_rate": 3.326898326898327e-05,
      "loss": 0.1469,
      "step": 2180
    },
    {
      "epoch": 0.3467384420519316,
      "grad_norm": 7.784045104238185,
      "learning_rate": 3.318854568854569e-05,
      "loss": 0.2058,
      "step": 2190
    },
    {
      "epoch": 0.3483217226092464,
      "grad_norm": 10.88816930594675,
      "learning_rate": 3.310810810810811e-05,
      "loss": 0.2224,
      "step": 2200
    },
    {
      "epoch": 0.3499050031665611,
      "grad_norm": 4.870308795201811,
      "learning_rate": 3.302767052767053e-05,
      "loss": 0.178,
      "step": 2210
    },
    {
      "epoch": 0.35148828372387586,
      "grad_norm": 0.3817859041258633,
      "learning_rate": 3.294723294723295e-05,
      "loss": 0.177,
      "step": 2220
    },
    {
      "epoch": 0.35307156428119063,
      "grad_norm": 4.496912002676331,
      "learning_rate": 3.286679536679537e-05,
      "loss": 0.1687,
      "step": 2230
    },
    {
      "epoch": 0.3546548448385054,
      "grad_norm": 8.964376519687175,
      "learning_rate": 3.2786357786357786e-05,
      "loss": 0.1231,
      "step": 2240
    },
    {
      "epoch": 0.3562381253958201,
      "grad_norm": 0.8847903486085875,
      "learning_rate": 3.270592020592021e-05,
      "loss": 0.2308,
      "step": 2250
    },
    {
      "epoch": 0.3578214059531349,
      "grad_norm": 26.470207309350638,
      "learning_rate": 3.262548262548263e-05,
      "loss": 0.2099,
      "step": 2260
    },
    {
      "epoch": 0.35940468651044966,
      "grad_norm": 9.06215019866392,
      "learning_rate": 3.254504504504505e-05,
      "loss": 0.2446,
      "step": 2270
    },
    {
      "epoch": 0.3609879670677644,
      "grad_norm": 11.858750318075058,
      "learning_rate": 3.2464607464607465e-05,
      "loss": 0.1627,
      "step": 2280
    },
    {
      "epoch": 0.36257124762507914,
      "grad_norm": 1.0151390086666994,
      "learning_rate": 3.238416988416988e-05,
      "loss": 0.1928,
      "step": 2290
    },
    {
      "epoch": 0.3641545281823939,
      "grad_norm": 16.203575135381968,
      "learning_rate": 3.23037323037323e-05,
      "loss": 0.2499,
      "step": 2300
    },
    {
      "epoch": 0.3657378087397087,
      "grad_norm": 1.753370989236795,
      "learning_rate": 3.222329472329472e-05,
      "loss": 0.1084,
      "step": 2310
    },
    {
      "epoch": 0.36732108929702345,
      "grad_norm": 9.048008770305184,
      "learning_rate": 3.2142857142857144e-05,
      "loss": 0.2056,
      "step": 2320
    },
    {
      "epoch": 0.36890436985433817,
      "grad_norm": 7.92814962573469,
      "learning_rate": 3.206241956241957e-05,
      "loss": 0.1935,
      "step": 2330
    },
    {
      "epoch": 0.37048765041165294,
      "grad_norm": 3.9171459364692627,
      "learning_rate": 3.198198198198199e-05,
      "loss": 0.172,
      "step": 2340
    },
    {
      "epoch": 0.3720709309689677,
      "grad_norm": 9.913225191198183,
      "learning_rate": 3.1901544401544405e-05,
      "loss": 0.1594,
      "step": 2350
    },
    {
      "epoch": 0.3736542115262825,
      "grad_norm": 9.162971399106288,
      "learning_rate": 3.182110682110682e-05,
      "loss": 0.1807,
      "step": 2360
    },
    {
      "epoch": 0.3752374920835972,
      "grad_norm": 1.95264111437521,
      "learning_rate": 3.174066924066924e-05,
      "loss": 0.1063,
      "step": 2370
    },
    {
      "epoch": 0.37682077264091196,
      "grad_norm": 10.67064385971434,
      "learning_rate": 3.166023166023166e-05,
      "loss": 0.1564,
      "step": 2380
    },
    {
      "epoch": 0.37840405319822673,
      "grad_norm": 5.495717028278223,
      "learning_rate": 3.1579794079794084e-05,
      "loss": 0.1863,
      "step": 2390
    },
    {
      "epoch": 0.3799873337555415,
      "grad_norm": 6.380689288844995,
      "learning_rate": 3.14993564993565e-05,
      "loss": 0.2454,
      "step": 2400
    },
    {
      "epoch": 0.3815706143128562,
      "grad_norm": 1.8147339701035865,
      "learning_rate": 3.141891891891892e-05,
      "loss": 0.1889,
      "step": 2410
    },
    {
      "epoch": 0.383153894870171,
      "grad_norm": 11.737291971623968,
      "learning_rate": 3.133848133848134e-05,
      "loss": 0.279,
      "step": 2420
    },
    {
      "epoch": 0.38473717542748576,
      "grad_norm": 10.556560584513866,
      "learning_rate": 3.1258043758043756e-05,
      "loss": 0.1151,
      "step": 2430
    },
    {
      "epoch": 0.3863204559848005,
      "grad_norm": 8.860008365119102,
      "learning_rate": 3.117760617760618e-05,
      "loss": 0.2661,
      "step": 2440
    },
    {
      "epoch": 0.38790373654211524,
      "grad_norm": 3.1344373489522543,
      "learning_rate": 3.10971685971686e-05,
      "loss": 0.1249,
      "step": 2450
    },
    {
      "epoch": 0.38948701709943,
      "grad_norm": 8.241088678235363,
      "learning_rate": 3.101673101673102e-05,
      "loss": 0.1998,
      "step": 2460
    },
    {
      "epoch": 0.3910702976567448,
      "grad_norm": 5.541466195440954,
      "learning_rate": 3.093629343629344e-05,
      "loss": 0.1883,
      "step": 2470
    },
    {
      "epoch": 0.39265357821405955,
      "grad_norm": 12.530672207494684,
      "learning_rate": 3.085585585585586e-05,
      "loss": 0.1692,
      "step": 2480
    },
    {
      "epoch": 0.39423685877137427,
      "grad_norm": 3.8658559645390325,
      "learning_rate": 3.077541827541828e-05,
      "loss": 0.1825,
      "step": 2490
    },
    {
      "epoch": 0.39582013932868904,
      "grad_norm": 1.0643886161645582,
      "learning_rate": 3.0694980694980696e-05,
      "loss": 0.1565,
      "step": 2500
    },
    {
      "epoch": 0.39582013932868904,
      "eval_loss": 0.2251758873462677,
      "eval_runtime": 303.7709,
      "eval_samples_per_second": 9.826,
      "eval_steps_per_second": 1.639,
      "step": 2500
    },
    {
      "epoch": 0.3974034198860038,
      "grad_norm": 2.213996415642763,
      "learning_rate": 3.0614543114543114e-05,
      "loss": 0.208,
      "step": 2510
    },
    {
      "epoch": 0.3989867004433186,
      "grad_norm": 3.965881151804943,
      "learning_rate": 3.053410553410553e-05,
      "loss": 0.1625,
      "step": 2520
    },
    {
      "epoch": 0.4005699810006333,
      "grad_norm": 7.067906293764895,
      "learning_rate": 3.0453667953667954e-05,
      "loss": 0.1934,
      "step": 2530
    },
    {
      "epoch": 0.40215326155794806,
      "grad_norm": 2.8249228036511256,
      "learning_rate": 3.037323037323038e-05,
      "loss": 0.1314,
      "step": 2540
    },
    {
      "epoch": 0.40373654211526283,
      "grad_norm": 0.1337776039952625,
      "learning_rate": 3.0292792792792797e-05,
      "loss": 0.2183,
      "step": 2550
    },
    {
      "epoch": 0.4053198226725776,
      "grad_norm": 5.729173177079634,
      "learning_rate": 3.0212355212355215e-05,
      "loss": 0.1696,
      "step": 2560
    },
    {
      "epoch": 0.4069031032298923,
      "grad_norm": 18.091377590311755,
      "learning_rate": 3.0131917631917633e-05,
      "loss": 0.1687,
      "step": 2570
    },
    {
      "epoch": 0.4084863837872071,
      "grad_norm": 3.403370331187315,
      "learning_rate": 3.005148005148005e-05,
      "loss": 0.1614,
      "step": 2580
    },
    {
      "epoch": 0.41006966434452186,
      "grad_norm": 3.058905068018445,
      "learning_rate": 2.9971042471042472e-05,
      "loss": 0.1643,
      "step": 2590
    },
    {
      "epoch": 0.4116529449018366,
      "grad_norm": 8.599049768514352,
      "learning_rate": 2.989060489060489e-05,
      "loss": 0.2086,
      "step": 2600
    },
    {
      "epoch": 0.41323622545915134,
      "grad_norm": 5.49214408933324,
      "learning_rate": 2.9810167310167315e-05,
      "loss": 0.125,
      "step": 2610
    },
    {
      "epoch": 0.4148195060164661,
      "grad_norm": 8.474881925857526,
      "learning_rate": 2.9729729729729733e-05,
      "loss": 0.2242,
      "step": 2620
    },
    {
      "epoch": 0.4164027865737809,
      "grad_norm": 0.5428084816177255,
      "learning_rate": 2.964929214929215e-05,
      "loss": 0.1654,
      "step": 2630
    },
    {
      "epoch": 0.41798606713109565,
      "grad_norm": 13.146633375743423,
      "learning_rate": 2.956885456885457e-05,
      "loss": 0.1803,
      "step": 2640
    },
    {
      "epoch": 0.41956934768841037,
      "grad_norm": 5.9291615611000985,
      "learning_rate": 2.948841698841699e-05,
      "loss": 0.1727,
      "step": 2650
    },
    {
      "epoch": 0.42115262824572514,
      "grad_norm": 9.626448485137793,
      "learning_rate": 2.940797940797941e-05,
      "loss": 0.1838,
      "step": 2660
    },
    {
      "epoch": 0.4227359088030399,
      "grad_norm": 5.193858650255415,
      "learning_rate": 2.9327541827541827e-05,
      "loss": 0.1125,
      "step": 2670
    },
    {
      "epoch": 0.4243191893603547,
      "grad_norm": 22.509334408152696,
      "learning_rate": 2.9247104247104252e-05,
      "loss": 0.3331,
      "step": 2680
    },
    {
      "epoch": 0.4259024699176694,
      "grad_norm": 6.338434106127838,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.304,
      "step": 2690
    },
    {
      "epoch": 0.42748575047498416,
      "grad_norm": 9.77118201267404,
      "learning_rate": 2.9086229086229088e-05,
      "loss": 0.1428,
      "step": 2700
    },
    {
      "epoch": 0.42906903103229893,
      "grad_norm": 1.2870263867403868,
      "learning_rate": 2.9005791505791506e-05,
      "loss": 0.16,
      "step": 2710
    },
    {
      "epoch": 0.4306523115896137,
      "grad_norm": 8.276686537441327,
      "learning_rate": 2.8925353925353927e-05,
      "loss": 0.1832,
      "step": 2720
    },
    {
      "epoch": 0.4322355921469284,
      "grad_norm": 6.8573612394072265,
      "learning_rate": 2.8844916344916345e-05,
      "loss": 0.1771,
      "step": 2730
    },
    {
      "epoch": 0.4338188727042432,
      "grad_norm": 17.17905065753462,
      "learning_rate": 2.8764478764478763e-05,
      "loss": 0.1344,
      "step": 2740
    },
    {
      "epoch": 0.43540215326155796,
      "grad_norm": 7.236953835970947,
      "learning_rate": 2.868404118404118e-05,
      "loss": 0.1884,
      "step": 2750
    },
    {
      "epoch": 0.4369854338188727,
      "grad_norm": 4.292366464589552,
      "learning_rate": 2.8603603603603606e-05,
      "loss": 0.1854,
      "step": 2760
    },
    {
      "epoch": 0.43856871437618744,
      "grad_norm": 11.219583379452915,
      "learning_rate": 2.8523166023166024e-05,
      "loss": 0.2342,
      "step": 2770
    },
    {
      "epoch": 0.4401519949335022,
      "grad_norm": 1.7529559375210721,
      "learning_rate": 2.8442728442728446e-05,
      "loss": 0.1636,
      "step": 2780
    },
    {
      "epoch": 0.441735275490817,
      "grad_norm": 20.230016573316014,
      "learning_rate": 2.8362290862290864e-05,
      "loss": 0.0911,
      "step": 2790
    },
    {
      "epoch": 0.44331855604813175,
      "grad_norm": 3.5956110903654723,
      "learning_rate": 2.8281853281853282e-05,
      "loss": 0.1968,
      "step": 2800
    },
    {
      "epoch": 0.44490183660544647,
      "grad_norm": 6.050112623947688,
      "learning_rate": 2.82014157014157e-05,
      "loss": 0.1685,
      "step": 2810
    },
    {
      "epoch": 0.44648511716276124,
      "grad_norm": 8.96037214868717,
      "learning_rate": 2.8120978120978118e-05,
      "loss": 0.176,
      "step": 2820
    },
    {
      "epoch": 0.448068397720076,
      "grad_norm": 0.26772209804984387,
      "learning_rate": 2.8040540540540543e-05,
      "loss": 0.2022,
      "step": 2830
    },
    {
      "epoch": 0.4496516782773908,
      "grad_norm": 6.741295747805091,
      "learning_rate": 2.7960102960102964e-05,
      "loss": 0.1473,
      "step": 2840
    },
    {
      "epoch": 0.4512349588347055,
      "grad_norm": 3.6863744278659913,
      "learning_rate": 2.7879665379665382e-05,
      "loss": 0.0842,
      "step": 2850
    },
    {
      "epoch": 0.45281823939202026,
      "grad_norm": 8.89123897745411,
      "learning_rate": 2.77992277992278e-05,
      "loss": 0.2065,
      "step": 2860
    },
    {
      "epoch": 0.45440151994933503,
      "grad_norm": 3.5907507280958395,
      "learning_rate": 2.771879021879022e-05,
      "loss": 0.1484,
      "step": 2870
    },
    {
      "epoch": 0.4559848005066498,
      "grad_norm": 4.036916845387968,
      "learning_rate": 2.7638352638352637e-05,
      "loss": 0.1083,
      "step": 2880
    },
    {
      "epoch": 0.4575680810639645,
      "grad_norm": 13.196805879636356,
      "learning_rate": 2.7557915057915058e-05,
      "loss": 0.1824,
      "step": 2890
    },
    {
      "epoch": 0.4591513616212793,
      "grad_norm": 2.5992298379380085,
      "learning_rate": 2.7477477477477483e-05,
      "loss": 0.1262,
      "step": 2900
    },
    {
      "epoch": 0.46073464217859406,
      "grad_norm": 3.999531956946186,
      "learning_rate": 2.73970398970399e-05,
      "loss": 0.1626,
      "step": 2910
    },
    {
      "epoch": 0.4623179227359088,
      "grad_norm": 7.749691864779105,
      "learning_rate": 2.731660231660232e-05,
      "loss": 0.1867,
      "step": 2920
    },
    {
      "epoch": 0.46390120329322354,
      "grad_norm": 6.969076243064114,
      "learning_rate": 2.7236164736164737e-05,
      "loss": 0.1415,
      "step": 2930
    },
    {
      "epoch": 0.4654844838505383,
      "grad_norm": 0.7107344117228024,
      "learning_rate": 2.7155727155727155e-05,
      "loss": 0.1618,
      "step": 2940
    },
    {
      "epoch": 0.4670677644078531,
      "grad_norm": 0.026148955919499367,
      "learning_rate": 2.7075289575289577e-05,
      "loss": 0.1442,
      "step": 2950
    },
    {
      "epoch": 0.46865104496516785,
      "grad_norm": 3.1797516837425643,
      "learning_rate": 2.6994851994851995e-05,
      "loss": 0.1001,
      "step": 2960
    },
    {
      "epoch": 0.47023432552248257,
      "grad_norm": 12.793716581195667,
      "learning_rate": 2.691441441441442e-05,
      "loss": 0.3138,
      "step": 2970
    },
    {
      "epoch": 0.47181760607979734,
      "grad_norm": 5.8830294018156115,
      "learning_rate": 2.6833976833976838e-05,
      "loss": 0.2802,
      "step": 2980
    },
    {
      "epoch": 0.4734008866371121,
      "grad_norm": 4.468059993371857,
      "learning_rate": 2.6753539253539256e-05,
      "loss": 0.1599,
      "step": 2990
    },
    {
      "epoch": 0.4749841671944269,
      "grad_norm": 11.923310959556547,
      "learning_rate": 2.6673101673101674e-05,
      "loss": 0.2076,
      "step": 3000
    },
    {
      "epoch": 0.4749841671944269,
      "eval_loss": 0.21503691375255585,
      "eval_runtime": 303.7768,
      "eval_samples_per_second": 9.826,
      "eval_steps_per_second": 1.639,
      "step": 3000
    },
    {
      "epoch": 0.4765674477517416,
      "grad_norm": 2.867039980718371,
      "learning_rate": 2.6592664092664095e-05,
      "loss": 0.1664,
      "step": 3010
    },
    {
      "epoch": 0.47815072830905636,
      "grad_norm": 4.021985905088124,
      "learning_rate": 2.6512226512226513e-05,
      "loss": 0.1755,
      "step": 3020
    },
    {
      "epoch": 0.47973400886637113,
      "grad_norm": 4.951465605500664,
      "learning_rate": 2.643178893178893e-05,
      "loss": 0.2248,
      "step": 3030
    },
    {
      "epoch": 0.4813172894236859,
      "grad_norm": 4.308265265413444,
      "learning_rate": 2.635135135135135e-05,
      "loss": 0.2324,
      "step": 3040
    },
    {
      "epoch": 0.4829005699810006,
      "grad_norm": 15.490383980057091,
      "learning_rate": 2.6270913770913774e-05,
      "loss": 0.1991,
      "step": 3050
    },
    {
      "epoch": 0.4844838505383154,
      "grad_norm": 13.182598920798608,
      "learning_rate": 2.6190476190476192e-05,
      "loss": 0.1494,
      "step": 3060
    },
    {
      "epoch": 0.48606713109563016,
      "grad_norm": 7.301560525140836,
      "learning_rate": 2.611003861003861e-05,
      "loss": 0.1205,
      "step": 3070
    },
    {
      "epoch": 0.4876504116529449,
      "grad_norm": 5.374603256842037,
      "learning_rate": 2.6029601029601032e-05,
      "loss": 0.2017,
      "step": 3080
    },
    {
      "epoch": 0.48923369221025964,
      "grad_norm": 18.470640237448457,
      "learning_rate": 2.594916344916345e-05,
      "loss": 0.237,
      "step": 3090
    },
    {
      "epoch": 0.4908169727675744,
      "grad_norm": 31.299031846548385,
      "learning_rate": 2.5868725868725868e-05,
      "loss": 0.1456,
      "step": 3100
    },
    {
      "epoch": 0.4924002533248892,
      "grad_norm": 8.659767290120616,
      "learning_rate": 2.5788288288288286e-05,
      "loss": 0.1738,
      "step": 3110
    },
    {
      "epoch": 0.49398353388220395,
      "grad_norm": 3.351028648843679,
      "learning_rate": 2.570785070785071e-05,
      "loss": 0.1909,
      "step": 3120
    },
    {
      "epoch": 0.49556681443951867,
      "grad_norm": 3.491878487315706,
      "learning_rate": 2.562741312741313e-05,
      "loss": 0.1946,
      "step": 3130
    },
    {
      "epoch": 0.49715009499683344,
      "grad_norm": 1.1366651070118903,
      "learning_rate": 2.554697554697555e-05,
      "loss": 0.2331,
      "step": 3140
    },
    {
      "epoch": 0.4987333755541482,
      "grad_norm": 11.897679565895796,
      "learning_rate": 2.546653796653797e-05,
      "loss": 0.1713,
      "step": 3150
    },
    {
      "epoch": 0.5003166561114629,
      "grad_norm": 12.557512346520221,
      "learning_rate": 2.5386100386100386e-05,
      "loss": 0.2024,
      "step": 3160
    },
    {
      "epoch": 0.5018999366687777,
      "grad_norm": 12.025712123412704,
      "learning_rate": 2.5305662805662804e-05,
      "loss": 0.2638,
      "step": 3170
    },
    {
      "epoch": 0.5034832172260925,
      "grad_norm": 1.5706290310353694,
      "learning_rate": 2.5225225225225222e-05,
      "loss": 0.2398,
      "step": 3180
    },
    {
      "epoch": 0.5050664977834072,
      "grad_norm": 0.8470662256537623,
      "learning_rate": 2.5144787644787647e-05,
      "loss": 0.1181,
      "step": 3190
    },
    {
      "epoch": 0.506649778340722,
      "grad_norm": 27.047968321706367,
      "learning_rate": 2.506435006435007e-05,
      "loss": 0.1875,
      "step": 3200
    },
    {
      "epoch": 0.5082330588980367,
      "grad_norm": 4.651981317221862,
      "learning_rate": 2.4983912483912487e-05,
      "loss": 0.16,
      "step": 3210
    },
    {
      "epoch": 0.5098163394553515,
      "grad_norm": 2.175113486475276,
      "learning_rate": 2.4903474903474905e-05,
      "loss": 0.1931,
      "step": 3220
    },
    {
      "epoch": 0.5113996200126663,
      "grad_norm": 0.19538203908854232,
      "learning_rate": 2.4823037323037323e-05,
      "loss": 0.1597,
      "step": 3230
    },
    {
      "epoch": 0.512982900569981,
      "grad_norm": 5.947978841676002,
      "learning_rate": 2.474259974259974e-05,
      "loss": 0.1771,
      "step": 3240
    },
    {
      "epoch": 0.5145661811272958,
      "grad_norm": 2.324322456161904,
      "learning_rate": 2.4662162162162162e-05,
      "loss": 0.1996,
      "step": 3250
    },
    {
      "epoch": 0.5161494616846105,
      "grad_norm": 9.914333378884143,
      "learning_rate": 2.4581724581724584e-05,
      "loss": 0.1233,
      "step": 3260
    },
    {
      "epoch": 0.5177327422419252,
      "grad_norm": 4.399245301459343,
      "learning_rate": 2.4501287001287002e-05,
      "loss": 0.157,
      "step": 3270
    },
    {
      "epoch": 0.51931602279924,
      "grad_norm": 1.947265135249437,
      "learning_rate": 2.4420849420849423e-05,
      "loss": 0.1742,
      "step": 3280
    },
    {
      "epoch": 0.5208993033565548,
      "grad_norm": 7.6922540618787565,
      "learning_rate": 2.434041184041184e-05,
      "loss": 0.1919,
      "step": 3290
    },
    {
      "epoch": 0.5224825839138696,
      "grad_norm": 9.128950126258486,
      "learning_rate": 2.425997425997426e-05,
      "loss": 0.1807,
      "step": 3300
    },
    {
      "epoch": 0.5240658644711843,
      "grad_norm": 0.783968316233508,
      "learning_rate": 2.417953667953668e-05,
      "loss": 0.1458,
      "step": 3310
    },
    {
      "epoch": 0.525649145028499,
      "grad_norm": 7.454664155391424,
      "learning_rate": 2.4099099099099102e-05,
      "loss": 0.1475,
      "step": 3320
    },
    {
      "epoch": 0.5272324255858138,
      "grad_norm": 0.37602138499816323,
      "learning_rate": 2.401866151866152e-05,
      "loss": 0.0957,
      "step": 3330
    },
    {
      "epoch": 0.5288157061431286,
      "grad_norm": 10.155932612228238,
      "learning_rate": 2.393822393822394e-05,
      "loss": 0.2575,
      "step": 3340
    },
    {
      "epoch": 0.5303989867004433,
      "grad_norm": 1.0704063318708195,
      "learning_rate": 2.385778635778636e-05,
      "loss": 0.1661,
      "step": 3350
    },
    {
      "epoch": 0.5319822672577581,
      "grad_norm": 3.415537081439389,
      "learning_rate": 2.3777348777348778e-05,
      "loss": 0.119,
      "step": 3360
    },
    {
      "epoch": 0.5335655478150728,
      "grad_norm": 9.30829417858808,
      "learning_rate": 2.36969111969112e-05,
      "loss": 0.1657,
      "step": 3370
    },
    {
      "epoch": 0.5351488283723876,
      "grad_norm": 7.807679176680901,
      "learning_rate": 2.3616473616473618e-05,
      "loss": 0.2805,
      "step": 3380
    },
    {
      "epoch": 0.5367321089297024,
      "grad_norm": 13.530264944117237,
      "learning_rate": 2.353603603603604e-05,
      "loss": 0.1726,
      "step": 3390
    },
    {
      "epoch": 0.5383153894870171,
      "grad_norm": 0.13336162087013698,
      "learning_rate": 2.3455598455598457e-05,
      "loss": 0.1229,
      "step": 3400
    },
    {
      "epoch": 0.5398986700443319,
      "grad_norm": 15.07172386794432,
      "learning_rate": 2.3375160875160875e-05,
      "loss": 0.1424,
      "step": 3410
    },
    {
      "epoch": 0.5414819506016466,
      "grad_norm": 0.06549848668701695,
      "learning_rate": 2.3294723294723293e-05,
      "loss": 0.0907,
      "step": 3420
    },
    {
      "epoch": 0.5430652311589613,
      "grad_norm": 2.933993417712982,
      "learning_rate": 2.3214285714285715e-05,
      "loss": 0.1992,
      "step": 3430
    },
    {
      "epoch": 0.5446485117162762,
      "grad_norm": 4.4457850950866185,
      "learning_rate": 2.3133848133848136e-05,
      "loss": 0.1132,
      "step": 3440
    },
    {
      "epoch": 0.5462317922735909,
      "grad_norm": 2.7842960607639404,
      "learning_rate": 2.3053410553410554e-05,
      "loss": 0.2321,
      "step": 3450
    },
    {
      "epoch": 0.5478150728309057,
      "grad_norm": 12.114564160337006,
      "learning_rate": 2.2972972972972976e-05,
      "loss": 0.1562,
      "step": 3460
    },
    {
      "epoch": 0.5493983533882204,
      "grad_norm": 9.919153323804045,
      "learning_rate": 2.2892535392535394e-05,
      "loss": 0.2976,
      "step": 3470
    },
    {
      "epoch": 0.5509816339455351,
      "grad_norm": 9.510489445073965,
      "learning_rate": 2.2812097812097812e-05,
      "loss": 0.1574,
      "step": 3480
    },
    {
      "epoch": 0.55256491450285,
      "grad_norm": 11.930438284741077,
      "learning_rate": 2.2731660231660233e-05,
      "loss": 0.1717,
      "step": 3490
    },
    {
      "epoch": 0.5541481950601647,
      "grad_norm": 13.248727989318859,
      "learning_rate": 2.2651222651222655e-05,
      "loss": 0.1119,
      "step": 3500
    },
    {
      "epoch": 0.5541481950601647,
      "eval_loss": 0.2323029786348343,
      "eval_runtime": 303.6903,
      "eval_samples_per_second": 9.829,
      "eval_steps_per_second": 1.64,
      "step": 3500
    },
    {
      "epoch": 0.5557314756174794,
      "grad_norm": 7.113379924494073,
      "learning_rate": 2.2570785070785073e-05,
      "loss": 0.1594,
      "step": 3510
    },
    {
      "epoch": 0.5573147561747942,
      "grad_norm": 2.5057124676832188,
      "learning_rate": 2.249034749034749e-05,
      "loss": 0.1842,
      "step": 3520
    },
    {
      "epoch": 0.5588980367321089,
      "grad_norm": 10.524100483129374,
      "learning_rate": 2.240990990990991e-05,
      "loss": 0.3038,
      "step": 3530
    },
    {
      "epoch": 0.5604813172894236,
      "grad_norm": 3.173891375571669,
      "learning_rate": 2.232947232947233e-05,
      "loss": 0.1427,
      "step": 3540
    },
    {
      "epoch": 0.5620645978467385,
      "grad_norm": 4.955889679365158,
      "learning_rate": 2.2249034749034752e-05,
      "loss": 0.2157,
      "step": 3550
    },
    {
      "epoch": 0.5636478784040532,
      "grad_norm": 16.804159031090713,
      "learning_rate": 2.216859716859717e-05,
      "loss": 0.161,
      "step": 3560
    },
    {
      "epoch": 0.565231158961368,
      "grad_norm": 7.354949851834345,
      "learning_rate": 2.208815958815959e-05,
      "loss": 0.1589,
      "step": 3570
    },
    {
      "epoch": 0.5668144395186827,
      "grad_norm": 6.0954146239001545,
      "learning_rate": 2.200772200772201e-05,
      "loss": 0.1867,
      "step": 3580
    },
    {
      "epoch": 0.5683977200759974,
      "grad_norm": 4.925133198907167,
      "learning_rate": 2.1927284427284427e-05,
      "loss": 0.1664,
      "step": 3590
    },
    {
      "epoch": 0.5699810006333123,
      "grad_norm": 3.720833331055052,
      "learning_rate": 2.1846846846846845e-05,
      "loss": 0.1464,
      "step": 3600
    },
    {
      "epoch": 0.571564281190627,
      "grad_norm": 6.750951629243882,
      "learning_rate": 2.1766409266409267e-05,
      "loss": 0.1727,
      "step": 3610
    },
    {
      "epoch": 0.5731475617479417,
      "grad_norm": 2.9534241862850146,
      "learning_rate": 2.168597168597169e-05,
      "loss": 0.1472,
      "step": 3620
    },
    {
      "epoch": 0.5747308423052565,
      "grad_norm": 0.8173760307878387,
      "learning_rate": 2.1605534105534106e-05,
      "loss": 0.219,
      "step": 3630
    },
    {
      "epoch": 0.5763141228625712,
      "grad_norm": 4.891140554145848,
      "learning_rate": 2.1525096525096524e-05,
      "loss": 0.1109,
      "step": 3640
    },
    {
      "epoch": 0.577897403419886,
      "grad_norm": 5.572212740449624,
      "learning_rate": 2.1444658944658946e-05,
      "loss": 0.1847,
      "step": 3650
    },
    {
      "epoch": 0.5794806839772008,
      "grad_norm": 1.6884291175014308,
      "learning_rate": 2.1364221364221364e-05,
      "loss": 0.2184,
      "step": 3660
    },
    {
      "epoch": 0.5810639645345155,
      "grad_norm": 8.656461527198916,
      "learning_rate": 2.1283783783783785e-05,
      "loss": 0.1834,
      "step": 3670
    },
    {
      "epoch": 0.5826472450918303,
      "grad_norm": 12.295887850506224,
      "learning_rate": 2.1203346203346207e-05,
      "loss": 0.1576,
      "step": 3680
    },
    {
      "epoch": 0.584230525649145,
      "grad_norm": 20.4707507371417,
      "learning_rate": 2.1122908622908625e-05,
      "loss": 0.2094,
      "step": 3690
    },
    {
      "epoch": 0.5858138062064597,
      "grad_norm": 8.273924844741206,
      "learning_rate": 2.1042471042471043e-05,
      "loss": 0.1766,
      "step": 3700
    },
    {
      "epoch": 0.5873970867637746,
      "grad_norm": 2.3695473569479146,
      "learning_rate": 2.096203346203346e-05,
      "loss": 0.2128,
      "step": 3710
    },
    {
      "epoch": 0.5889803673210893,
      "grad_norm": 15.947629095865532,
      "learning_rate": 2.0881595881595882e-05,
      "loss": 0.16,
      "step": 3720
    },
    {
      "epoch": 0.5905636478784041,
      "grad_norm": 6.645041040849207,
      "learning_rate": 2.08011583011583e-05,
      "loss": 0.1166,
      "step": 3730
    },
    {
      "epoch": 0.5921469284357188,
      "grad_norm": 5.686145327701723,
      "learning_rate": 2.0720720720720722e-05,
      "loss": 0.164,
      "step": 3740
    },
    {
      "epoch": 0.5937302089930335,
      "grad_norm": 9.495621726364991,
      "learning_rate": 2.0640283140283143e-05,
      "loss": 0.159,
      "step": 3750
    },
    {
      "epoch": 0.5953134895503484,
      "grad_norm": 4.0028585710092175,
      "learning_rate": 2.055984555984556e-05,
      "loss": 0.1099,
      "step": 3760
    },
    {
      "epoch": 0.5968967701076631,
      "grad_norm": 5.215861726191841,
      "learning_rate": 2.047940797940798e-05,
      "loss": 0.1331,
      "step": 3770
    },
    {
      "epoch": 0.5984800506649778,
      "grad_norm": 9.713395093914809,
      "learning_rate": 2.0398970398970398e-05,
      "loss": 0.1748,
      "step": 3780
    },
    {
      "epoch": 0.6000633312222926,
      "grad_norm": 1.8987304084776646,
      "learning_rate": 2.031853281853282e-05,
      "loss": 0.1388,
      "step": 3790
    },
    {
      "epoch": 0.6016466117796073,
      "grad_norm": 0.6439571945302127,
      "learning_rate": 2.023809523809524e-05,
      "loss": 0.1699,
      "step": 3800
    },
    {
      "epoch": 0.6032298923369221,
      "grad_norm": 17.745336645334458,
      "learning_rate": 2.015765765765766e-05,
      "loss": 0.1592,
      "step": 3810
    },
    {
      "epoch": 0.6048131728942369,
      "grad_norm": 5.347865076216217,
      "learning_rate": 2.0077220077220077e-05,
      "loss": 0.2896,
      "step": 3820
    },
    {
      "epoch": 0.6063964534515516,
      "grad_norm": 11.11455259643614,
      "learning_rate": 1.9996782496782498e-05,
      "loss": 0.1586,
      "step": 3830
    },
    {
      "epoch": 0.6079797340088664,
      "grad_norm": 14.670873443251109,
      "learning_rate": 1.9916344916344916e-05,
      "loss": 0.1971,
      "step": 3840
    },
    {
      "epoch": 0.6095630145661811,
      "grad_norm": 4.3469830014861985,
      "learning_rate": 1.9835907335907338e-05,
      "loss": 0.207,
      "step": 3850
    },
    {
      "epoch": 0.6111462951234958,
      "grad_norm": 1.585366959195709,
      "learning_rate": 1.975546975546976e-05,
      "loss": 0.1125,
      "step": 3860
    },
    {
      "epoch": 0.6127295756808107,
      "grad_norm": 1.1122979891527625,
      "learning_rate": 1.9675032175032177e-05,
      "loss": 0.0985,
      "step": 3870
    },
    {
      "epoch": 0.6143128562381254,
      "grad_norm": 29.292135033225,
      "learning_rate": 1.9594594594594595e-05,
      "loss": 0.2111,
      "step": 3880
    },
    {
      "epoch": 0.6158961367954402,
      "grad_norm": 20.195413744910624,
      "learning_rate": 1.9514157014157013e-05,
      "loss": 0.1707,
      "step": 3890
    },
    {
      "epoch": 0.6174794173527549,
      "grad_norm": 0.21371583503824718,
      "learning_rate": 1.9433719433719435e-05,
      "loss": 0.0798,
      "step": 3900
    },
    {
      "epoch": 0.6190626979100696,
      "grad_norm": 7.687938646657218,
      "learning_rate": 1.9353281853281853e-05,
      "loss": 0.1646,
      "step": 3910
    },
    {
      "epoch": 0.6206459784673845,
      "grad_norm": 11.601251442589273,
      "learning_rate": 1.9272844272844274e-05,
      "loss": 0.1806,
      "step": 3920
    },
    {
      "epoch": 0.6222292590246992,
      "grad_norm": 20.627769700394175,
      "learning_rate": 1.9192406692406692e-05,
      "loss": 0.1182,
      "step": 3930
    },
    {
      "epoch": 0.6238125395820139,
      "grad_norm": 6.105908905779207,
      "learning_rate": 1.9111969111969114e-05,
      "loss": 0.2164,
      "step": 3940
    },
    {
      "epoch": 0.6253958201393287,
      "grad_norm": 3.0765533307637773,
      "learning_rate": 1.9031531531531532e-05,
      "loss": 0.1049,
      "step": 3950
    },
    {
      "epoch": 0.6269791006966434,
      "grad_norm": 8.593532490145114,
      "learning_rate": 1.895109395109395e-05,
      "loss": 0.1441,
      "step": 3960
    },
    {
      "epoch": 0.6285623812539582,
      "grad_norm": 0.3511451787422904,
      "learning_rate": 1.887065637065637e-05,
      "loss": 0.1304,
      "step": 3970
    },
    {
      "epoch": 0.630145661811273,
      "grad_norm": 7.696819827787694,
      "learning_rate": 1.8790218790218793e-05,
      "loss": 0.1424,
      "step": 3980
    },
    {
      "epoch": 0.6317289423685877,
      "grad_norm": 9.972567505970206,
      "learning_rate": 1.870978120978121e-05,
      "loss": 0.1413,
      "step": 3990
    },
    {
      "epoch": 0.6333122229259025,
      "grad_norm": 17.059305304171605,
      "learning_rate": 1.862934362934363e-05,
      "loss": 0.2486,
      "step": 4000
    },
    {
      "epoch": 0.6333122229259025,
      "eval_loss": 0.2179439663887024,
      "eval_runtime": 303.3246,
      "eval_samples_per_second": 9.841,
      "eval_steps_per_second": 1.642,
      "step": 4000
    },
    {
      "epoch": 0.6348955034832172,
      "grad_norm": 4.409948866151987,
      "learning_rate": 1.854890604890605e-05,
      "loss": 0.0708,
      "step": 4010
    },
    {
      "epoch": 0.6364787840405319,
      "grad_norm": 7.007854414764561,
      "learning_rate": 1.846846846846847e-05,
      "loss": 0.227,
      "step": 4020
    },
    {
      "epoch": 0.6380620645978468,
      "grad_norm": 6.003142805336199,
      "learning_rate": 1.838803088803089e-05,
      "loss": 0.2006,
      "step": 4030
    },
    {
      "epoch": 0.6396453451551615,
      "grad_norm": 8.213680429844667,
      "learning_rate": 1.830759330759331e-05,
      "loss": 0.1118,
      "step": 4040
    },
    {
      "epoch": 0.6412286257124763,
      "grad_norm": 2.3169622927887357,
      "learning_rate": 1.822715572715573e-05,
      "loss": 0.1544,
      "step": 4050
    },
    {
      "epoch": 0.642811906269791,
      "grad_norm": 0.4907634509386203,
      "learning_rate": 1.8146718146718147e-05,
      "loss": 0.063,
      "step": 4060
    },
    {
      "epoch": 0.6443951868271057,
      "grad_norm": 2.0274832205307374,
      "learning_rate": 1.8066280566280565e-05,
      "loss": 0.2615,
      "step": 4070
    },
    {
      "epoch": 0.6459784673844206,
      "grad_norm": 7.005768170492898,
      "learning_rate": 1.7985842985842987e-05,
      "loss": 0.187,
      "step": 4080
    },
    {
      "epoch": 0.6475617479417353,
      "grad_norm": 19.323873288174447,
      "learning_rate": 1.7905405405405405e-05,
      "loss": 0.2426,
      "step": 4090
    },
    {
      "epoch": 0.64914502849905,
      "grad_norm": 3.4795859230813595,
      "learning_rate": 1.7824967824967826e-05,
      "loss": 0.2537,
      "step": 4100
    },
    {
      "epoch": 0.6507283090563648,
      "grad_norm": 1.2607746193967093,
      "learning_rate": 1.7744530244530244e-05,
      "loss": 0.1063,
      "step": 4110
    },
    {
      "epoch": 0.6523115896136795,
      "grad_norm": 9.160800883500768,
      "learning_rate": 1.7664092664092666e-05,
      "loss": 0.1367,
      "step": 4120
    },
    {
      "epoch": 0.6538948701709943,
      "grad_norm": 23.24952352712376,
      "learning_rate": 1.7583655083655084e-05,
      "loss": 0.1544,
      "step": 4130
    },
    {
      "epoch": 0.6554781507283091,
      "grad_norm": 4.722219442229606,
      "learning_rate": 1.7503217503217502e-05,
      "loss": 0.0825,
      "step": 4140
    },
    {
      "epoch": 0.6570614312856238,
      "grad_norm": 6.105143532905107,
      "learning_rate": 1.7422779922779923e-05,
      "loss": 0.1886,
      "step": 4150
    },
    {
      "epoch": 0.6586447118429386,
      "grad_norm": 1.7037684773641748,
      "learning_rate": 1.7342342342342345e-05,
      "loss": 0.1691,
      "step": 4160
    },
    {
      "epoch": 0.6602279924002533,
      "grad_norm": 2.8819803830603576,
      "learning_rate": 1.7261904761904763e-05,
      "loss": 0.1464,
      "step": 4170
    },
    {
      "epoch": 0.661811272957568,
      "grad_norm": 0.6009032116975369,
      "learning_rate": 1.718146718146718e-05,
      "loss": 0.119,
      "step": 4180
    },
    {
      "epoch": 0.6633945535148829,
      "grad_norm": 5.231798135532088,
      "learning_rate": 1.7101029601029602e-05,
      "loss": 0.1453,
      "step": 4190
    },
    {
      "epoch": 0.6649778340721976,
      "grad_norm": 0.5983672058413865,
      "learning_rate": 1.702059202059202e-05,
      "loss": 0.1824,
      "step": 4200
    },
    {
      "epoch": 0.6665611146295124,
      "grad_norm": 6.466135574764903,
      "learning_rate": 1.6940154440154442e-05,
      "loss": 0.1941,
      "step": 4210
    },
    {
      "epoch": 0.6681443951868271,
      "grad_norm": 0.41586315244327504,
      "learning_rate": 1.685971685971686e-05,
      "loss": 0.069,
      "step": 4220
    },
    {
      "epoch": 0.6697276757441418,
      "grad_norm": 16.735452740155218,
      "learning_rate": 1.677927927927928e-05,
      "loss": 0.2355,
      "step": 4230
    },
    {
      "epoch": 0.6713109563014567,
      "grad_norm": 0.15602785053977397,
      "learning_rate": 1.66988416988417e-05,
      "loss": 0.041,
      "step": 4240
    },
    {
      "epoch": 0.6728942368587714,
      "grad_norm": 13.976352884118455,
      "learning_rate": 1.6618404118404118e-05,
      "loss": 0.2254,
      "step": 4250
    },
    {
      "epoch": 0.6744775174160861,
      "grad_norm": 3.1342363808007305,
      "learning_rate": 1.653796653796654e-05,
      "loss": 0.2303,
      "step": 4260
    },
    {
      "epoch": 0.6760607979734009,
      "grad_norm": 7.691931215023337,
      "learning_rate": 1.6457528957528957e-05,
      "loss": 0.1694,
      "step": 4270
    },
    {
      "epoch": 0.6776440785307156,
      "grad_norm": 1.103855479072457,
      "learning_rate": 1.637709137709138e-05,
      "loss": 0.1236,
      "step": 4280
    },
    {
      "epoch": 0.6792273590880304,
      "grad_norm": 9.681722776469597,
      "learning_rate": 1.6296653796653797e-05,
      "loss": 0.1853,
      "step": 4290
    },
    {
      "epoch": 0.6808106396453452,
      "grad_norm": 0.7638431756954955,
      "learning_rate": 1.6216216216216218e-05,
      "loss": 0.1325,
      "step": 4300
    },
    {
      "epoch": 0.6823939202026599,
      "grad_norm": 25.79023754328491,
      "learning_rate": 1.6135778635778636e-05,
      "loss": 0.1956,
      "step": 4310
    },
    {
      "epoch": 0.6839772007599747,
      "grad_norm": 12.845416107149674,
      "learning_rate": 1.6055341055341054e-05,
      "loss": 0.1929,
      "step": 4320
    },
    {
      "epoch": 0.6855604813172894,
      "grad_norm": 1.0497573345061644,
      "learning_rate": 1.5974903474903476e-05,
      "loss": 0.0655,
      "step": 4330
    },
    {
      "epoch": 0.6871437618746041,
      "grad_norm": 2.2589509727017787,
      "learning_rate": 1.5894465894465897e-05,
      "loss": 0.288,
      "step": 4340
    },
    {
      "epoch": 0.688727042431919,
      "grad_norm": 0.3541925598477795,
      "learning_rate": 1.5814028314028315e-05,
      "loss": 0.1857,
      "step": 4350
    },
    {
      "epoch": 0.6903103229892337,
      "grad_norm": 5.3493881374821335,
      "learning_rate": 1.5733590733590733e-05,
      "loss": 0.1542,
      "step": 4360
    },
    {
      "epoch": 0.6918936035465485,
      "grad_norm": 26.008827471558813,
      "learning_rate": 1.5653153153153155e-05,
      "loss": 0.1696,
      "step": 4370
    },
    {
      "epoch": 0.6934768841038632,
      "grad_norm": 1.308203112242577,
      "learning_rate": 1.5572715572715573e-05,
      "loss": 0.1125,
      "step": 4380
    },
    {
      "epoch": 0.6950601646611779,
      "grad_norm": 2.4180078950095196,
      "learning_rate": 1.5492277992277994e-05,
      "loss": 0.1156,
      "step": 4390
    },
    {
      "epoch": 0.6966434452184928,
      "grad_norm": 21.87962562701028,
      "learning_rate": 1.5411840411840412e-05,
      "loss": 0.1668,
      "step": 4400
    },
    {
      "epoch": 0.6982267257758075,
      "grad_norm": 5.974086752334123,
      "learning_rate": 1.5331402831402834e-05,
      "loss": 0.1972,
      "step": 4410
    },
    {
      "epoch": 0.6998100063331222,
      "grad_norm": 19.646700190340887,
      "learning_rate": 1.5250965250965252e-05,
      "loss": 0.2638,
      "step": 4420
    },
    {
      "epoch": 0.701393286890437,
      "grad_norm": 15.998122105068203,
      "learning_rate": 1.517052767052767e-05,
      "loss": 0.2685,
      "step": 4430
    },
    {
      "epoch": 0.7029765674477517,
      "grad_norm": 3.0238125196398444,
      "learning_rate": 1.5090090090090091e-05,
      "loss": 0.2027,
      "step": 4440
    },
    {
      "epoch": 0.7045598480050665,
      "grad_norm": 3.852672115295891,
      "learning_rate": 1.5009652509652511e-05,
      "loss": 0.1011,
      "step": 4450
    },
    {
      "epoch": 0.7061431285623813,
      "grad_norm": 0.5205505525315096,
      "learning_rate": 1.4929214929214929e-05,
      "loss": 0.1942,
      "step": 4460
    },
    {
      "epoch": 0.707726409119696,
      "grad_norm": 4.433795941149514,
      "learning_rate": 1.4848777348777349e-05,
      "loss": 0.1413,
      "step": 4470
    },
    {
      "epoch": 0.7093096896770108,
      "grad_norm": 0.7709042842963173,
      "learning_rate": 1.476833976833977e-05,
      "loss": 0.174,
      "step": 4480
    },
    {
      "epoch": 0.7108929702343255,
      "grad_norm": 6.56059600820765,
      "learning_rate": 1.4687902187902188e-05,
      "loss": 0.1465,
      "step": 4490
    },
    {
      "epoch": 0.7124762507916402,
      "grad_norm": 10.794749248043004,
      "learning_rate": 1.4607464607464608e-05,
      "loss": 0.1573,
      "step": 4500
    },
    {
      "epoch": 0.7124762507916402,
      "eval_loss": 0.20407366752624512,
      "eval_runtime": 302.9611,
      "eval_samples_per_second": 9.853,
      "eval_steps_per_second": 1.644,
      "step": 4500
    },
    {
      "epoch": 0.7140595313489551,
      "grad_norm": 5.967527254295678,
      "learning_rate": 1.4527027027027026e-05,
      "loss": 0.1438,
      "step": 4510
    },
    {
      "epoch": 0.7156428119062698,
      "grad_norm": 0.6834372859094578,
      "learning_rate": 1.4446589446589448e-05,
      "loss": 0.1238,
      "step": 4520
    },
    {
      "epoch": 0.7172260924635846,
      "grad_norm": 0.5659599913957317,
      "learning_rate": 1.4366151866151867e-05,
      "loss": 0.1078,
      "step": 4530
    },
    {
      "epoch": 0.7188093730208993,
      "grad_norm": 3.7513656672613025,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.1607,
      "step": 4540
    },
    {
      "epoch": 0.720392653578214,
      "grad_norm": 7.914468828259802,
      "learning_rate": 1.4205276705276707e-05,
      "loss": 0.141,
      "step": 4550
    },
    {
      "epoch": 0.7219759341355289,
      "grad_norm": 9.882445413760555,
      "learning_rate": 1.4124839124839127e-05,
      "loss": 0.1979,
      "step": 4560
    },
    {
      "epoch": 0.7235592146928436,
      "grad_norm": 10.487011096275165,
      "learning_rate": 1.4044401544401545e-05,
      "loss": 0.1742,
      "step": 4570
    },
    {
      "epoch": 0.7251424952501583,
      "grad_norm": 0.24219118392357505,
      "learning_rate": 1.3963963963963963e-05,
      "loss": 0.2081,
      "step": 4580
    },
    {
      "epoch": 0.7267257758074731,
      "grad_norm": 9.893186409297137,
      "learning_rate": 1.3883526383526386e-05,
      "loss": 0.1822,
      "step": 4590
    },
    {
      "epoch": 0.7283090563647878,
      "grad_norm": 8.053340943608678,
      "learning_rate": 1.3803088803088804e-05,
      "loss": 0.2274,
      "step": 4600
    },
    {
      "epoch": 0.7298923369221026,
      "grad_norm": 16.864580116326643,
      "learning_rate": 1.3722651222651222e-05,
      "loss": 0.2343,
      "step": 4610
    },
    {
      "epoch": 0.7314756174794174,
      "grad_norm": 8.836345470936402,
      "learning_rate": 1.3642213642213642e-05,
      "loss": 0.1905,
      "step": 4620
    },
    {
      "epoch": 0.7330588980367321,
      "grad_norm": 0.34918251831054314,
      "learning_rate": 1.3561776061776063e-05,
      "loss": 0.1077,
      "step": 4630
    },
    {
      "epoch": 0.7346421785940469,
      "grad_norm": 5.62983275603831,
      "learning_rate": 1.3481338481338481e-05,
      "loss": 0.1884,
      "step": 4640
    },
    {
      "epoch": 0.7362254591513616,
      "grad_norm": 9.299892416711192,
      "learning_rate": 1.3400900900900901e-05,
      "loss": 0.1123,
      "step": 4650
    },
    {
      "epoch": 0.7378087397086763,
      "grad_norm": 5.413702442257127,
      "learning_rate": 1.3320463320463322e-05,
      "loss": 0.1496,
      "step": 4660
    },
    {
      "epoch": 0.7393920202659912,
      "grad_norm": 0.8226588525395829,
      "learning_rate": 1.324002574002574e-05,
      "loss": 0.1528,
      "step": 4670
    },
    {
      "epoch": 0.7409753008233059,
      "grad_norm": 8.284220612539634,
      "learning_rate": 1.315958815958816e-05,
      "loss": 0.2966,
      "step": 4680
    },
    {
      "epoch": 0.7425585813806207,
      "grad_norm": 7.428856451204816,
      "learning_rate": 1.3079150579150578e-05,
      "loss": 0.1708,
      "step": 4690
    },
    {
      "epoch": 0.7441418619379354,
      "grad_norm": 10.764409897749962,
      "learning_rate": 1.2998712998713e-05,
      "loss": 0.1435,
      "step": 4700
    },
    {
      "epoch": 0.7457251424952501,
      "grad_norm": 8.23643210266061,
      "learning_rate": 1.291827541827542e-05,
      "loss": 0.1507,
      "step": 4710
    },
    {
      "epoch": 0.747308423052565,
      "grad_norm": 5.402106312287174,
      "learning_rate": 1.2837837837837838e-05,
      "loss": 0.158,
      "step": 4720
    },
    {
      "epoch": 0.7488917036098797,
      "grad_norm": 1.0293773641368074,
      "learning_rate": 1.2757400257400259e-05,
      "loss": 0.1892,
      "step": 4730
    },
    {
      "epoch": 0.7504749841671944,
      "grad_norm": 0.4966315260745682,
      "learning_rate": 1.2676962676962679e-05,
      "loss": 0.2027,
      "step": 4740
    },
    {
      "epoch": 0.7520582647245092,
      "grad_norm": 0.6965451191051032,
      "learning_rate": 1.2596525096525097e-05,
      "loss": 0.1081,
      "step": 4750
    },
    {
      "epoch": 0.7536415452818239,
      "grad_norm": 0.3999187573242282,
      "learning_rate": 1.2516087516087515e-05,
      "loss": 0.2016,
      "step": 4760
    },
    {
      "epoch": 0.7552248258391387,
      "grad_norm": 24.371884816961735,
      "learning_rate": 1.2435649935649936e-05,
      "loss": 0.1149,
      "step": 4770
    },
    {
      "epoch": 0.7568081063964535,
      "grad_norm": 7.519699927085402,
      "learning_rate": 1.2355212355212356e-05,
      "loss": 0.0779,
      "step": 4780
    },
    {
      "epoch": 0.7583913869537682,
      "grad_norm": 4.329372845838211,
      "learning_rate": 1.2274774774774774e-05,
      "loss": 0.1104,
      "step": 4790
    },
    {
      "epoch": 0.759974667511083,
      "grad_norm": 9.308076973039874,
      "learning_rate": 1.2194337194337196e-05,
      "loss": 0.1176,
      "step": 4800
    },
    {
      "epoch": 0.7615579480683977,
      "grad_norm": 11.073936628350845,
      "learning_rate": 1.2113899613899614e-05,
      "loss": 0.1926,
      "step": 4810
    },
    {
      "epoch": 0.7631412286257124,
      "grad_norm": 8.596467077574387,
      "learning_rate": 1.2033462033462033e-05,
      "loss": 0.1597,
      "step": 4820
    },
    {
      "epoch": 0.7647245091830273,
      "grad_norm": 11.335218684136072,
      "learning_rate": 1.1953024453024455e-05,
      "loss": 0.1783,
      "step": 4830
    },
    {
      "epoch": 0.766307789740342,
      "grad_norm": 6.219074796696394,
      "learning_rate": 1.1872586872586873e-05,
      "loss": 0.1187,
      "step": 4840
    },
    {
      "epoch": 0.7678910702976568,
      "grad_norm": 1.9757834848170315,
      "learning_rate": 1.1792149292149293e-05,
      "loss": 0.2018,
      "step": 4850
    },
    {
      "epoch": 0.7694743508549715,
      "grad_norm": 4.177825912464328,
      "learning_rate": 1.1711711711711713e-05,
      "loss": 0.1139,
      "step": 4860
    },
    {
      "epoch": 0.7710576314122862,
      "grad_norm": 0.5251593098300185,
      "learning_rate": 1.1631274131274132e-05,
      "loss": 0.1902,
      "step": 4870
    },
    {
      "epoch": 0.772640911969601,
      "grad_norm": 9.755929953135594,
      "learning_rate": 1.155083655083655e-05,
      "loss": 0.1148,
      "step": 4880
    },
    {
      "epoch": 0.7742241925269158,
      "grad_norm": 13.402441092804466,
      "learning_rate": 1.1470398970398972e-05,
      "loss": 0.1942,
      "step": 4890
    },
    {
      "epoch": 0.7758074730842305,
      "grad_norm": 4.312695927246514,
      "learning_rate": 1.138996138996139e-05,
      "loss": 0.1259,
      "step": 4900
    },
    {
      "epoch": 0.7773907536415453,
      "grad_norm": 19.448181161442353,
      "learning_rate": 1.130952380952381e-05,
      "loss": 0.2607,
      "step": 4910
    },
    {
      "epoch": 0.77897403419886,
      "grad_norm": 5.821939985861638,
      "learning_rate": 1.1229086229086231e-05,
      "loss": 0.174,
      "step": 4920
    },
    {
      "epoch": 0.7805573147561748,
      "grad_norm": 19.269012178164857,
      "learning_rate": 1.1148648648648649e-05,
      "loss": 0.2352,
      "step": 4930
    },
    {
      "epoch": 0.7821405953134896,
      "grad_norm": 8.823639883210003,
      "learning_rate": 1.1068211068211069e-05,
      "loss": 0.2591,
      "step": 4940
    },
    {
      "epoch": 0.7837238758708043,
      "grad_norm": 9.32526971258217,
      "learning_rate": 1.0987773487773489e-05,
      "loss": 0.2759,
      "step": 4950
    },
    {
      "epoch": 0.7853071564281191,
      "grad_norm": 2.207729316974803,
      "learning_rate": 1.0907335907335908e-05,
      "loss": 0.1511,
      "step": 4960
    },
    {
      "epoch": 0.7868904369854338,
      "grad_norm": 6.82256756149058,
      "learning_rate": 1.0826898326898326e-05,
      "loss": 0.2232,
      "step": 4970
    },
    {
      "epoch": 0.7884737175427485,
      "grad_norm": 9.243961322011877,
      "learning_rate": 1.0746460746460748e-05,
      "loss": 0.1646,
      "step": 4980
    },
    {
      "epoch": 0.7900569981000634,
      "grad_norm": 15.917253626574382,
      "learning_rate": 1.0666023166023166e-05,
      "loss": 0.1524,
      "step": 4990
    },
    {
      "epoch": 0.7916402786573781,
      "grad_norm": 16.428390937903618,
      "learning_rate": 1.0585585585585586e-05,
      "loss": 0.1877,
      "step": 5000
    },
    {
      "epoch": 0.7916402786573781,
      "eval_loss": 0.18776774406433105,
      "eval_runtime": 303.073,
      "eval_samples_per_second": 9.849,
      "eval_steps_per_second": 1.643,
      "step": 5000
    },
    {
      "epoch": 0.7932235592146929,
      "grad_norm": 4.383738047744212,
      "learning_rate": 1.0505148005148005e-05,
      "loss": 0.1163,
      "step": 5010
    },
    {
      "epoch": 0.7948068397720076,
      "grad_norm": 0.34366321551943907,
      "learning_rate": 1.0424710424710425e-05,
      "loss": 0.0992,
      "step": 5020
    },
    {
      "epoch": 0.7963901203293223,
      "grad_norm": 4.840631477839234,
      "learning_rate": 1.0344272844272845e-05,
      "loss": 0.1474,
      "step": 5030
    },
    {
      "epoch": 0.7979734008866372,
      "grad_norm": 10.385839475992961,
      "learning_rate": 1.0263835263835265e-05,
      "loss": 0.1086,
      "step": 5040
    },
    {
      "epoch": 0.7995566814439519,
      "grad_norm": 19.493793771492218,
      "learning_rate": 1.0183397683397684e-05,
      "loss": 0.1594,
      "step": 5050
    },
    {
      "epoch": 0.8011399620012666,
      "grad_norm": 15.107558100751362,
      "learning_rate": 1.0102960102960103e-05,
      "loss": 0.1442,
      "step": 5060
    },
    {
      "epoch": 0.8027232425585814,
      "grad_norm": 25.670161946420766,
      "learning_rate": 1.0022522522522524e-05,
      "loss": 0.1631,
      "step": 5070
    },
    {
      "epoch": 0.8043065231158961,
      "grad_norm": 10.319259740021629,
      "learning_rate": 9.942084942084942e-06,
      "loss": 0.216,
      "step": 5080
    },
    {
      "epoch": 0.805889803673211,
      "grad_norm": 0.9589354896904739,
      "learning_rate": 9.861647361647362e-06,
      "loss": 0.1334,
      "step": 5090
    },
    {
      "epoch": 0.8074730842305257,
      "grad_norm": 4.273653516592426,
      "learning_rate": 9.781209781209782e-06,
      "loss": 0.0542,
      "step": 5100
    },
    {
      "epoch": 0.8090563647878404,
      "grad_norm": 23.236402904104477,
      "learning_rate": 9.700772200772201e-06,
      "loss": 0.1692,
      "step": 5110
    },
    {
      "epoch": 0.8106396453451552,
      "grad_norm": 13.30112770290344,
      "learning_rate": 9.620334620334621e-06,
      "loss": 0.2032,
      "step": 5120
    },
    {
      "epoch": 0.8122229259024699,
      "grad_norm": 17.912206381436427,
      "learning_rate": 9.53989703989704e-06,
      "loss": 0.2193,
      "step": 5130
    },
    {
      "epoch": 0.8138062064597846,
      "grad_norm": 3.853512778960239,
      "learning_rate": 9.45945945945946e-06,
      "loss": 0.1034,
      "step": 5140
    },
    {
      "epoch": 0.8153894870170995,
      "grad_norm": 12.406809604428807,
      "learning_rate": 9.379021879021879e-06,
      "loss": 0.1953,
      "step": 5150
    },
    {
      "epoch": 0.8169727675744142,
      "grad_norm": 4.449382293347965,
      "learning_rate": 9.2985842985843e-06,
      "loss": 0.2071,
      "step": 5160
    },
    {
      "epoch": 0.818556048131729,
      "grad_norm": 4.016711611252263,
      "learning_rate": 9.218146718146718e-06,
      "loss": 0.1681,
      "step": 5170
    },
    {
      "epoch": 0.8201393286890437,
      "grad_norm": 5.024417384140058,
      "learning_rate": 9.137709137709138e-06,
      "loss": 0.3021,
      "step": 5180
    },
    {
      "epoch": 0.8217226092463584,
      "grad_norm": 1.8551460708317054,
      "learning_rate": 9.057271557271558e-06,
      "loss": 0.117,
      "step": 5190
    },
    {
      "epoch": 0.8233058898036733,
      "grad_norm": 10.471670170496585,
      "learning_rate": 8.976833976833977e-06,
      "loss": 0.169,
      "step": 5200
    },
    {
      "epoch": 0.824889170360988,
      "grad_norm": 4.242963688270894,
      "learning_rate": 8.896396396396395e-06,
      "loss": 0.193,
      "step": 5210
    },
    {
      "epoch": 0.8264724509183027,
      "grad_norm": 2.8380072320976346,
      "learning_rate": 8.815958815958817e-06,
      "loss": 0.1396,
      "step": 5220
    },
    {
      "epoch": 0.8280557314756175,
      "grad_norm": 0.540864944131786,
      "learning_rate": 8.735521235521237e-06,
      "loss": 0.1947,
      "step": 5230
    },
    {
      "epoch": 0.8296390120329322,
      "grad_norm": 4.934034845805119,
      "learning_rate": 8.655083655083655e-06,
      "loss": 0.2245,
      "step": 5240
    },
    {
      "epoch": 0.831222292590247,
      "grad_norm": 7.177805324904442,
      "learning_rate": 8.574646074646076e-06,
      "loss": 0.309,
      "step": 5250
    },
    {
      "epoch": 0.8328055731475618,
      "grad_norm": 19.884288992329314,
      "learning_rate": 8.494208494208494e-06,
      "loss": 0.1802,
      "step": 5260
    },
    {
      "epoch": 0.8343888537048765,
      "grad_norm": 4.988434385191054,
      "learning_rate": 8.413770913770914e-06,
      "loss": 0.1656,
      "step": 5270
    },
    {
      "epoch": 0.8359721342621913,
      "grad_norm": 7.5326043015332065,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.192,
      "step": 5280
    },
    {
      "epoch": 0.837555414819506,
      "grad_norm": 0.1246712429400174,
      "learning_rate": 8.252895752895753e-06,
      "loss": 0.2299,
      "step": 5290
    },
    {
      "epoch": 0.8391386953768207,
      "grad_norm": 16.528201884038424,
      "learning_rate": 8.172458172458172e-06,
      "loss": 0.132,
      "step": 5300
    },
    {
      "epoch": 0.8407219759341356,
      "grad_norm": 6.670929531314776,
      "learning_rate": 8.092020592020593e-06,
      "loss": 0.1952,
      "step": 5310
    },
    {
      "epoch": 0.8423052564914503,
      "grad_norm": 2.8091039711440056,
      "learning_rate": 8.011583011583013e-06,
      "loss": 0.0623,
      "step": 5320
    },
    {
      "epoch": 0.843888537048765,
      "grad_norm": 8.986080001260978,
      "learning_rate": 7.93114543114543e-06,
      "loss": 0.1461,
      "step": 5330
    },
    {
      "epoch": 0.8454718176060798,
      "grad_norm": 0.5351171548861335,
      "learning_rate": 7.850707850707852e-06,
      "loss": 0.0671,
      "step": 5340
    },
    {
      "epoch": 0.8470550981633945,
      "grad_norm": 8.182170035763168,
      "learning_rate": 7.77027027027027e-06,
      "loss": 0.1511,
      "step": 5350
    },
    {
      "epoch": 0.8486383787207094,
      "grad_norm": 0.32337659594109197,
      "learning_rate": 7.68983268983269e-06,
      "loss": 0.1243,
      "step": 5360
    },
    {
      "epoch": 0.8502216592780241,
      "grad_norm": 6.052952750477383,
      "learning_rate": 7.609395109395109e-06,
      "loss": 0.2378,
      "step": 5370
    },
    {
      "epoch": 0.8518049398353388,
      "grad_norm": 3.9391982034370794,
      "learning_rate": 7.52895752895753e-06,
      "loss": 0.1931,
      "step": 5380
    },
    {
      "epoch": 0.8533882203926536,
      "grad_norm": 9.98765641377638,
      "learning_rate": 7.4485199485199485e-06,
      "loss": 0.2182,
      "step": 5390
    },
    {
      "epoch": 0.8549715009499683,
      "grad_norm": 3.0113051708166525,
      "learning_rate": 7.368082368082368e-06,
      "loss": 0.1734,
      "step": 5400
    },
    {
      "epoch": 0.856554781507283,
      "grad_norm": 1.327530133036487,
      "learning_rate": 7.287644787644787e-06,
      "loss": 0.2182,
      "step": 5410
    },
    {
      "epoch": 0.8581380620645979,
      "grad_norm": 11.807970904430736,
      "learning_rate": 7.207207207207208e-06,
      "loss": 0.1356,
      "step": 5420
    },
    {
      "epoch": 0.8597213426219126,
      "grad_norm": 1.401386497444269,
      "learning_rate": 7.1267696267696275e-06,
      "loss": 0.131,
      "step": 5430
    },
    {
      "epoch": 0.8613046231792274,
      "grad_norm": 3.210941509606301,
      "learning_rate": 7.046332046332046e-06,
      "loss": 0.1026,
      "step": 5440
    },
    {
      "epoch": 0.8628879037365421,
      "grad_norm": 6.23076263021643,
      "learning_rate": 6.965894465894467e-06,
      "loss": 0.2155,
      "step": 5450
    },
    {
      "epoch": 0.8644711842938568,
      "grad_norm": 5.108189996613664,
      "learning_rate": 6.885456885456885e-06,
      "loss": 0.1784,
      "step": 5460
    },
    {
      "epoch": 0.8660544648511717,
      "grad_norm": 7.249411460402617,
      "learning_rate": 6.805019305019306e-06,
      "loss": 0.1537,
      "step": 5470
    },
    {
      "epoch": 0.8676377454084864,
      "grad_norm": 3.1410886507686,
      "learning_rate": 6.724581724581725e-06,
      "loss": 0.1476,
      "step": 5480
    },
    {
      "epoch": 0.8692210259658011,
      "grad_norm": 2.8508512263074293,
      "learning_rate": 6.644144144144144e-06,
      "loss": 0.256,
      "step": 5490
    },
    {
      "epoch": 0.8708043065231159,
      "grad_norm": 5.062453281516686,
      "learning_rate": 6.563706563706563e-06,
      "loss": 0.1723,
      "step": 5500
    },
    {
      "epoch": 0.8708043065231159,
      "eval_loss": 0.19015242159366608,
      "eval_runtime": 303.3067,
      "eval_samples_per_second": 9.842,
      "eval_steps_per_second": 1.642,
      "step": 5500
    },
    {
      "epoch": 0.8723875870804306,
      "grad_norm": 9.42673590987627,
      "learning_rate": 6.483268983268984e-06,
      "loss": 0.1588,
      "step": 5510
    },
    {
      "epoch": 0.8739708676377455,
      "grad_norm": 5.313989228593378,
      "learning_rate": 6.402831402831404e-06,
      "loss": 0.2121,
      "step": 5520
    },
    {
      "epoch": 0.8755541481950602,
      "grad_norm": 4.459231823852883,
      "learning_rate": 6.3223938223938225e-06,
      "loss": 0.1802,
      "step": 5530
    },
    {
      "epoch": 0.8771374287523749,
      "grad_norm": 11.926712674485227,
      "learning_rate": 6.241956241956242e-06,
      "loss": 0.1106,
      "step": 5540
    },
    {
      "epoch": 0.8787207093096897,
      "grad_norm": 8.96381776876902,
      "learning_rate": 6.161518661518661e-06,
      "loss": 0.1461,
      "step": 5550
    },
    {
      "epoch": 0.8803039898670044,
      "grad_norm": 0.5909473881532172,
      "learning_rate": 6.081081081081082e-06,
      "loss": 0.1255,
      "step": 5560
    },
    {
      "epoch": 0.8818872704243191,
      "grad_norm": 6.4181815336759325,
      "learning_rate": 6.0006435006435015e-06,
      "loss": 0.1051,
      "step": 5570
    },
    {
      "epoch": 0.883470550981634,
      "grad_norm": 0.9389210421482053,
      "learning_rate": 5.9202059202059204e-06,
      "loss": 0.1071,
      "step": 5580
    },
    {
      "epoch": 0.8850538315389487,
      "grad_norm": 14.396821815718727,
      "learning_rate": 5.83976833976834e-06,
      "loss": 0.2409,
      "step": 5590
    },
    {
      "epoch": 0.8866371120962635,
      "grad_norm": 18.598346661334073,
      "learning_rate": 5.75933075933076e-06,
      "loss": 0.176,
      "step": 5600
    },
    {
      "epoch": 0.8882203926535782,
      "grad_norm": 0.2283224773059614,
      "learning_rate": 5.678893178893179e-06,
      "loss": 0.1393,
      "step": 5610
    },
    {
      "epoch": 0.8898036732108929,
      "grad_norm": 4.329543559484265,
      "learning_rate": 5.598455598455599e-06,
      "loss": 0.2101,
      "step": 5620
    },
    {
      "epoch": 0.8913869537682078,
      "grad_norm": 8.458873662618252,
      "learning_rate": 5.518018018018018e-06,
      "loss": 0.1829,
      "step": 5630
    },
    {
      "epoch": 0.8929702343255225,
      "grad_norm": 1.6641499670977171,
      "learning_rate": 5.437580437580437e-06,
      "loss": 0.1552,
      "step": 5640
    },
    {
      "epoch": 0.8945535148828372,
      "grad_norm": 2.855742768839621,
      "learning_rate": 5.357142857142857e-06,
      "loss": 0.1748,
      "step": 5650
    },
    {
      "epoch": 0.896136795440152,
      "grad_norm": 13.968528797011148,
      "learning_rate": 5.276705276705278e-06,
      "loss": 0.0717,
      "step": 5660
    },
    {
      "epoch": 0.8977200759974667,
      "grad_norm": 45.0382798880401,
      "learning_rate": 5.1962676962676965e-06,
      "loss": 0.1517,
      "step": 5670
    },
    {
      "epoch": 0.8993033565547816,
      "grad_norm": 0.23510271584622738,
      "learning_rate": 5.115830115830116e-06,
      "loss": 0.1419,
      "step": 5680
    },
    {
      "epoch": 0.9008866371120963,
      "grad_norm": 13.2604248494345,
      "learning_rate": 5.035392535392536e-06,
      "loss": 0.1223,
      "step": 5690
    },
    {
      "epoch": 0.902469917669411,
      "grad_norm": 7.616646865830118,
      "learning_rate": 4.954954954954955e-06,
      "loss": 0.1653,
      "step": 5700
    },
    {
      "epoch": 0.9040531982267258,
      "grad_norm": 2.9800898759845,
      "learning_rate": 4.874517374517375e-06,
      "loss": 0.1625,
      "step": 5710
    },
    {
      "epoch": 0.9056364787840405,
      "grad_norm": 13.040432775137894,
      "learning_rate": 4.7940797940797945e-06,
      "loss": 0.1838,
      "step": 5720
    },
    {
      "epoch": 0.9072197593413552,
      "grad_norm": 6.8927012665218355,
      "learning_rate": 4.713642213642213e-06,
      "loss": 0.1089,
      "step": 5730
    },
    {
      "epoch": 0.9088030398986701,
      "grad_norm": 6.304586757152827,
      "learning_rate": 4.633204633204633e-06,
      "loss": 0.1486,
      "step": 5740
    },
    {
      "epoch": 0.9103863204559848,
      "grad_norm": 6.170861578685023,
      "learning_rate": 4.552767052767053e-06,
      "loss": 0.2311,
      "step": 5750
    },
    {
      "epoch": 0.9119696010132996,
      "grad_norm": 14.115925532450762,
      "learning_rate": 4.472329472329473e-06,
      "loss": 0.1803,
      "step": 5760
    },
    {
      "epoch": 0.9135528815706143,
      "grad_norm": 1.118246203741243,
      "learning_rate": 4.391891891891892e-06,
      "loss": 0.1352,
      "step": 5770
    },
    {
      "epoch": 0.915136162127929,
      "grad_norm": 9.460786700129729,
      "learning_rate": 4.311454311454312e-06,
      "loss": 0.1237,
      "step": 5780
    },
    {
      "epoch": 0.9167194426852439,
      "grad_norm": 9.92937285902464,
      "learning_rate": 4.231016731016731e-06,
      "loss": 0.1218,
      "step": 5790
    },
    {
      "epoch": 0.9183027232425586,
      "grad_norm": 0.8758167133756707,
      "learning_rate": 4.150579150579151e-06,
      "loss": 0.1571,
      "step": 5800
    },
    {
      "epoch": 0.9198860037998733,
      "grad_norm": 15.192258991880278,
      "learning_rate": 4.0701415701415706e-06,
      "loss": 0.1924,
      "step": 5810
    },
    {
      "epoch": 0.9214692843571881,
      "grad_norm": 4.002486648108415,
      "learning_rate": 3.9897039897039895e-06,
      "loss": 0.1521,
      "step": 5820
    },
    {
      "epoch": 0.9230525649145028,
      "grad_norm": 0.057289287624309564,
      "learning_rate": 3.909266409266409e-06,
      "loss": 0.1606,
      "step": 5830
    },
    {
      "epoch": 0.9246358454718177,
      "grad_norm": 9.791645022970316,
      "learning_rate": 3.828828828828829e-06,
      "loss": 0.1602,
      "step": 5840
    },
    {
      "epoch": 0.9262191260291324,
      "grad_norm": 1.300919075843894,
      "learning_rate": 3.7483912483912483e-06,
      "loss": 0.1519,
      "step": 5850
    },
    {
      "epoch": 0.9278024065864471,
      "grad_norm": 3.201416304207286,
      "learning_rate": 3.6679536679536685e-06,
      "loss": 0.1306,
      "step": 5860
    },
    {
      "epoch": 0.9293856871437619,
      "grad_norm": 9.711424983364672,
      "learning_rate": 3.587516087516088e-06,
      "loss": 0.208,
      "step": 5870
    },
    {
      "epoch": 0.9309689677010766,
      "grad_norm": 4.735944330455318,
      "learning_rate": 3.5070785070785076e-06,
      "loss": 0.1838,
      "step": 5880
    },
    {
      "epoch": 0.9325522482583913,
      "grad_norm": 10.38744738183311,
      "learning_rate": 3.426640926640927e-06,
      "loss": 0.1621,
      "step": 5890
    },
    {
      "epoch": 0.9341355288157062,
      "grad_norm": 4.617435558387822,
      "learning_rate": 3.3462033462033462e-06,
      "loss": 0.1982,
      "step": 5900
    },
    {
      "epoch": 0.9357188093730209,
      "grad_norm": 7.963365598187153,
      "learning_rate": 3.265765765765766e-06,
      "loss": 0.1106,
      "step": 5910
    },
    {
      "epoch": 0.9373020899303357,
      "grad_norm": 6.2175282567344246,
      "learning_rate": 3.1853281853281853e-06,
      "loss": 0.1213,
      "step": 5920
    },
    {
      "epoch": 0.9388853704876504,
      "grad_norm": 6.547651659070995,
      "learning_rate": 3.104890604890605e-06,
      "loss": 0.098,
      "step": 5930
    },
    {
      "epoch": 0.9404686510449651,
      "grad_norm": 9.488962286437728,
      "learning_rate": 3.024453024453025e-06,
      "loss": 0.1178,
      "step": 5940
    },
    {
      "epoch": 0.94205193160228,
      "grad_norm": 10.595885115084549,
      "learning_rate": 2.944015444015444e-06,
      "loss": 0.1429,
      "step": 5950
    },
    {
      "epoch": 0.9436352121595947,
      "grad_norm": 1.144538280478487,
      "learning_rate": 2.8635778635778635e-06,
      "loss": 0.2007,
      "step": 5960
    },
    {
      "epoch": 0.9452184927169094,
      "grad_norm": 4.473555535275858,
      "learning_rate": 2.7831402831402833e-06,
      "loss": 0.2659,
      "step": 5970
    },
    {
      "epoch": 0.9468017732742242,
      "grad_norm": 0.10547527098806245,
      "learning_rate": 2.702702702702703e-06,
      "loss": 0.2236,
      "step": 5980
    },
    {
      "epoch": 0.9483850538315389,
      "grad_norm": 13.026465253362876,
      "learning_rate": 2.6222651222651223e-06,
      "loss": 0.1224,
      "step": 5990
    },
    {
      "epoch": 0.9499683343888538,
      "grad_norm": 6.809858457463955,
      "learning_rate": 2.541827541827542e-06,
      "loss": 0.1464,
      "step": 6000
    },
    {
      "epoch": 0.9499683343888538,
      "eval_loss": 0.2051822394132614,
      "eval_runtime": 303.5798,
      "eval_samples_per_second": 9.833,
      "eval_steps_per_second": 1.64,
      "step": 6000
    },
    {
      "epoch": 0.9515516149461685,
      "grad_norm": 10.006856856329383,
      "learning_rate": 2.4613899613899614e-06,
      "loss": 0.1445,
      "step": 6010
    },
    {
      "epoch": 0.9531348955034832,
      "grad_norm": 0.2579481028352667,
      "learning_rate": 2.3809523809523808e-06,
      "loss": 0.1104,
      "step": 6020
    },
    {
      "epoch": 0.954718176060798,
      "grad_norm": 5.924155246696603,
      "learning_rate": 2.300514800514801e-06,
      "loss": 0.188,
      "step": 6030
    },
    {
      "epoch": 0.9563014566181127,
      "grad_norm": 18.289734109037695,
      "learning_rate": 2.2200772200772203e-06,
      "loss": 0.2712,
      "step": 6040
    },
    {
      "epoch": 0.9578847371754274,
      "grad_norm": 1.360211476755544,
      "learning_rate": 2.1396396396396396e-06,
      "loss": 0.0848,
      "step": 6050
    },
    {
      "epoch": 0.9594680177327423,
      "grad_norm": 20.91641239249807,
      "learning_rate": 2.0592020592020594e-06,
      "loss": 0.2548,
      "step": 6060
    },
    {
      "epoch": 0.961051298290057,
      "grad_norm": 5.1951331545054735,
      "learning_rate": 1.9787644787644787e-06,
      "loss": 0.1234,
      "step": 6070
    },
    {
      "epoch": 0.9626345788473718,
      "grad_norm": 3.1497080773613315,
      "learning_rate": 1.8983268983268987e-06,
      "loss": 0.2487,
      "step": 6080
    },
    {
      "epoch": 0.9642178594046865,
      "grad_norm": 9.081712864042878,
      "learning_rate": 1.817889317889318e-06,
      "loss": 0.135,
      "step": 6090
    },
    {
      "epoch": 0.9658011399620012,
      "grad_norm": 7.2042013096532616,
      "learning_rate": 1.7374517374517375e-06,
      "loss": 0.2192,
      "step": 6100
    },
    {
      "epoch": 0.9673844205193161,
      "grad_norm": 0.2061554088572478,
      "learning_rate": 1.657014157014157e-06,
      "loss": 0.1276,
      "step": 6110
    },
    {
      "epoch": 0.9689677010766308,
      "grad_norm": 0.12352345151070598,
      "learning_rate": 1.5765765765765764e-06,
      "loss": 0.099,
      "step": 6120
    },
    {
      "epoch": 0.9705509816339455,
      "grad_norm": 9.396797170212064,
      "learning_rate": 1.4961389961389962e-06,
      "loss": 0.0922,
      "step": 6130
    },
    {
      "epoch": 0.9721342621912603,
      "grad_norm": 13.44199257203487,
      "learning_rate": 1.415701415701416e-06,
      "loss": 0.1075,
      "step": 6140
    },
    {
      "epoch": 0.973717542748575,
      "grad_norm": 4.486435898452451,
      "learning_rate": 1.3352638352638352e-06,
      "loss": 0.1753,
      "step": 6150
    },
    {
      "epoch": 0.9753008233058899,
      "grad_norm": 4.719780411542333,
      "learning_rate": 1.2548262548262548e-06,
      "loss": 0.2045,
      "step": 6160
    },
    {
      "epoch": 0.9768841038632046,
      "grad_norm": 4.0245734231596915,
      "learning_rate": 1.1743886743886745e-06,
      "loss": 0.1808,
      "step": 6170
    },
    {
      "epoch": 0.9784673844205193,
      "grad_norm": 14.251167115719198,
      "learning_rate": 1.0939510939510939e-06,
      "loss": 0.1072,
      "step": 6180
    },
    {
      "epoch": 0.9800506649778341,
      "grad_norm": 8.095949323382486,
      "learning_rate": 1.0135135135135136e-06,
      "loss": 0.1625,
      "step": 6190
    },
    {
      "epoch": 0.9816339455351488,
      "grad_norm": 13.128052565409549,
      "learning_rate": 9.330759330759331e-07,
      "loss": 0.1475,
      "step": 6200
    },
    {
      "epoch": 0.9832172260924635,
      "grad_norm": 6.395568278864603,
      "learning_rate": 8.526383526383526e-07,
      "loss": 0.0912,
      "step": 6210
    },
    {
      "epoch": 0.9848005066497784,
      "grad_norm": 3.6032517582084416,
      "learning_rate": 7.722007722007723e-07,
      "loss": 0.1999,
      "step": 6220
    },
    {
      "epoch": 0.9863837872070931,
      "grad_norm": 6.315928519542067,
      "learning_rate": 6.917631917631918e-07,
      "loss": 0.2017,
      "step": 6230
    },
    {
      "epoch": 0.9879670677644079,
      "grad_norm": 12.091013675219127,
      "learning_rate": 6.113256113256113e-07,
      "loss": 0.1657,
      "step": 6240
    },
    {
      "epoch": 0.9895503483217226,
      "grad_norm": 0.7157394967249527,
      "learning_rate": 5.308880308880309e-07,
      "loss": 0.1333,
      "step": 6250
    },
    {
      "epoch": 0.9911336288790373,
      "grad_norm": 0.3832637112908861,
      "learning_rate": 4.504504504504505e-07,
      "loss": 0.1237,
      "step": 6260
    },
    {
      "epoch": 0.9927169094363522,
      "grad_norm": 9.909429479164734,
      "learning_rate": 3.7001287001287003e-07,
      "loss": 0.2039,
      "step": 6270
    },
    {
      "epoch": 0.9943001899936669,
      "grad_norm": 8.766820815825819,
      "learning_rate": 2.8957528957528957e-07,
      "loss": 0.1577,
      "step": 6280
    },
    {
      "epoch": 0.9958834705509816,
      "grad_norm": 19.548181723408607,
      "learning_rate": 2.0913770913770914e-07,
      "loss": 0.2518,
      "step": 6290
    },
    {
      "epoch": 0.9974667511082964,
      "grad_norm": 0.9153900287649347,
      "learning_rate": 1.287001287001287e-07,
      "loss": 0.2023,
      "step": 6300
    },
    {
      "epoch": 0.9990500316656111,
      "grad_norm": 10.06251630130182,
      "learning_rate": 4.8262548262548266e-08,
      "loss": 0.1466,
      "step": 6310
    }
  ],
  "logging_steps": 10,
  "max_steps": 6316,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 486697973317632.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
